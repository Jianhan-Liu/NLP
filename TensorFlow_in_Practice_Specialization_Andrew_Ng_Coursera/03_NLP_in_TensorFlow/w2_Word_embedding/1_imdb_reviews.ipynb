{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.enable_eager_execution()\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1005 13:27:18.200756 14244 dataset_builder.py:439] Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHQ2Ko0zl7M4"
   },
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "    \n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())\n",
    "\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(1e4, oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=120, truncating='post')\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = {value: key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in its many forms kids may be the target audience but it 's fun for everyone to laugh at its comical silliness yet at the same time root for the good guys to <OOV> and save the world the acting is cheesy in places but that is the charm there are several lines of corny dialog possibly translation errors or possibly intentional jokes by the movie makers and you 'll find yourself <OOV> these absurd lines later br br admittedly this film is not high in production quality or budget however for what it is campy sci fi it 's enjoyable for some laughs i recommend it to anyone with a sense of humor for that sort of thing '\n",
      "b'If you love Japanese monster movies, you\\'ll love this action packed battle pitting an alien invader, intent upon conquering the Earth, and a \"Giant Robot\" with an armory of super weaponry. The alien, \"Emporer Guillotine,\" from the planet Gargoyle, has a army of thugs called, (of course) \"the gargoyle gang,\" as well as an endless supply of immense hostile creatures that are routinely loosed upon the Earth to smash buildings, make loud noises, panic the populace, etc. A little kid, named Johnny Sokko, has the Giant Robot at his beckon call, and sends the Robot, as needed, to beat up, and then blast these creatures. Johnny joins a group of \"good spies\" called Unicorn, and endeavors to help save the world.<br /><br />In spite of the campy nature, unintentionally humorous dialog, and the fact that the target audience was obviously children, this movie has non-stop action, colorful characters, decent special effects, and just happens to be downright fun to watch. Battle scenes are well executed, and frequent, as the storyline requires. The good guys and bad guys both made sure they had an inexhaustible supply of bombs, lasers, ammunition, and schemes to attack each other. In spite of the fact the movie was constructed from edited episodes of a TV series, the plot actually develops, and reaches an ultimate conclusion.<br /><br />The film has a positive outlook and appeals to everyone\\'s (especially kids\\') desire to destroy evil in its many forms. Kids may be the target audience, but it\\'s fun for everyone to laugh at its comical silliness; yet, at the same time, root for the good guys to prevail and \"save the world.\" The acting is cheesy in places, but that is the charm: there are several lines of corny dialog (possibly translation errors or possibly intentional jokes by the movie makers), and you\\'ll find yourself quoting these absurd lines later.<br /><br />Admittedly, this film is not high in production quality or budget. However, for what it is, campy sci-fi, it\\'s enjoyable for some laughs. I recommend it to anyone with a sense of humor for that sort of thing.'\n"
     ]
    }
   ],
   "source": [
    "print(decode_review(padded[1]))\n",
    "print(testing_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16, input_length=120),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 - 3s - loss: 9.5898e-05 - acc: 1.0000 - val_loss: 0.8295 - val_acc: 0.8275\n",
      "Epoch 2/10\n",
      "25000/25000 - 3s - loss: 5.9991e-05 - acc: 1.0000 - val_loss: 0.8625 - val_acc: 0.8275\n",
      "Epoch 3/10\n",
      "25000/25000 - 3s - loss: 3.7705e-05 - acc: 1.0000 - val_loss: 0.8951 - val_acc: 0.8274\n",
      "Epoch 4/10\n",
      "25000/25000 - 2s - loss: 2.3444e-05 - acc: 1.0000 - val_loss: 0.9280 - val_acc: 0.8278\n",
      "Epoch 5/10\n",
      "25000/25000 - 2s - loss: 1.5192e-05 - acc: 1.0000 - val_loss: 0.9570 - val_acc: 0.8271\n",
      "Epoch 6/10\n",
      "25000/25000 - 2s - loss: 9.3921e-06 - acc: 1.0000 - val_loss: 0.9882 - val_acc: 0.8274\n",
      "Epoch 7/10\n",
      "25000/25000 - 2s - loss: 5.9938e-06 - acc: 1.0000 - val_loss: 1.0156 - val_acc: 0.8273\n",
      "Epoch 8/10\n",
      "25000/25000 - 2s - loss: 3.8241e-06 - acc: 1.0000 - val_loss: 1.0426 - val_acc: 0.8282\n",
      "Epoch 9/10\n",
      "25000/25000 - 2s - loss: 2.4383e-06 - acc: 1.0000 - val_loss: 1.0685 - val_acc: 0.8276\n",
      "Epoch 10/10\n",
      "25000/25000 - 2s - loss: 1.5302e-06 - acc: 1.0000 - val_loss: 1.0952 - val_acc: 0.8276\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded, training_labels_final, epochs=10, \n",
    "          validation_data=(testing_padded, testing_labels_final),\n",
    "         verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = e.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, 10000):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word+'\\n')\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
