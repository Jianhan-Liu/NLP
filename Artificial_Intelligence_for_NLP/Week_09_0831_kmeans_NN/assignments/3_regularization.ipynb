{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_print(y_true, y_pred):\n",
    "    if len(np.shape(y_true)) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(np.shape(y_pred)) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(X_train=train_dataset, y_train=train_labels, \n",
    "                X_test=test_dataset, y_test=test_labels,\n",
    "                learning_rate = .1,\n",
    "                dropout = False,\n",
    "                dropout_rate = .5,\n",
    "                nn=True,\n",
    "                multiple_nn = False,\n",
    "                regularizer=0., \n",
    "                epochs=2, \n",
    "                batch_size=128,\n",
    "                optimizer=keras.optimizers.SGD,\n",
    "                callback = [],\n",
    "                validation_data=(valid_dataset, valid_labels)):\n",
    "\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(784,)))\n",
    "    if nn:\n",
    "        if multiple_nn:\n",
    "            model.add(keras.layers.Dense(100, activation=tf.nn.relu,\n",
    "                                        kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "            model.add(keras.layers.Dense(100, activation=tf.nn.relu,\n",
    "                                        kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "        model.add(keras.layers.Dense(1024, activation=tf.nn.relu, \n",
    "                                     kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "        if dropout:\n",
    "            model.add(keras.layers.Dropout(dropout_rate))\n",
    "    model.add(keras.layers.Dense(10, activation=tf.nn.softmax, \n",
    "                                 kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "    \n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=callback)\n",
    "    print(f'Test accuracy: {model.evaluate(X_test, y_test, verbose=0)[1]*100:.2f}% with regularizer: {regularizer}')\n",
    "    classification_print(y_test, model.predict(X_test))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 5s 26us/sample - loss: 0.6886 - acc: 0.8182 - val_loss: 0.6576 - val_acc: 0.8272\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 4s 18us/sample - loss: 0.6408 - acc: 0.8300 - val_loss: 0.6502 - val_acc: 0.8292\n",
      "Test accuracy: 89.61% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92      1000\n",
      "           1       0.88      0.90      0.89      1000\n",
      "           2       0.90      0.93      0.92      1000\n",
      "           3       0.93      0.91      0.92      1000\n",
      "           4       0.90      0.86      0.88      1000\n",
      "           5       0.87      0.93      0.90      1000\n",
      "           6       0.90      0.89      0.89      1000\n",
      "           7       0.92      0.89      0.90      1000\n",
      "           8       0.87      0.84      0.86      1000\n",
      "           9       0.87      0.90      0.88      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_91 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logistic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 0.8282 - acc: 0.8146 - val_loss: 0.7857 - val_acc: 0.8202\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 3s 17us/sample - loss: 0.7795 - acc: 0.8214 - val_loss: 0.7874 - val_acc: 0.8186\n",
      "Test accuracy: 88.85% with regularizer:0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91      1000\n",
      "           1       0.93      0.87      0.90      1000\n",
      "           2       0.91      0.93      0.92      1000\n",
      "           3       0.91      0.91      0.91      1000\n",
      "           4       0.91      0.86      0.89      1000\n",
      "           5       0.91      0.91      0.91      1000\n",
      "           6       0.90      0.89      0.90      1000\n",
      "           7       0.92      0.87      0.89      1000\n",
      "           8       0.80      0.86      0.83      1000\n",
      "           9       0.80      0.89      0.85      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False, regularizer=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 4s 18us/sample - loss: 0.7152 - acc: 0.8181 - val_loss: 0.6869 - val_acc: 0.8262\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 3s 15us/sample - loss: 0.6696 - acc: 0.8290 - val_loss: 0.6739 - val_acc: 0.8304\n",
      "Test accuracy: 89.71% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91      1000\n",
      "           1       0.92      0.89      0.90      1000\n",
      "           2       0.89      0.93      0.91      1000\n",
      "           3       0.91      0.93      0.92      1000\n",
      "           4       0.90      0.87      0.88      1000\n",
      "           5       0.92      0.92      0.92      1000\n",
      "           6       0.91      0.89      0.90      1000\n",
      "           7       0.90      0.89      0.90      1000\n",
      "           8       0.84      0.85      0.85      1000\n",
      "           9       0.86      0.90      0.88      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False, regularizer=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.9211 - acc: 0.7070 - val_loss: 1.9044 - val_acc: 0.6929\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 3s 15us/sample - loss: 1.8839 - acc: 0.7074 - val_loss: 1.8995 - val_acc: 0.6621\n",
      "Test accuracy: 72.71% with regularizer:1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.68      0.79      1000\n",
      "           1       0.79      0.74      0.76      1000\n",
      "           2       0.55      0.89      0.68      1000\n",
      "           3       0.89      0.80      0.84      1000\n",
      "           4       0.87      0.66      0.75      1000\n",
      "           5       0.93      0.76      0.84      1000\n",
      "           6       0.96      0.36      0.52      1000\n",
      "           7       0.92      0.66      0.77      1000\n",
      "           8       0.69      0.83      0.75      1000\n",
      "           9       0.45      0.89      0.60      1000\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.80      0.73      0.73     10000\n",
      "weighted avg       0.80      0.73      0.73     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False, regularizer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 17s 83us/sample - loss: 0.7475 - acc: 0.7993 - val_loss: 0.6105 - val_acc: 0.8295\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 16s 78us/sample - loss: 0.5760 - acc: 0.8369 - val_loss: 0.5637 - val_acc: 0.8406\n",
      "Test accuracy: 90.85% with regularizer:0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1000\n",
      "           1       0.91      0.89      0.90      1000\n",
      "           2       0.91      0.94      0.93      1000\n",
      "           3       0.93      0.92      0.92      1000\n",
      "           4       0.93      0.88      0.90      1000\n",
      "           5       0.92      0.93      0.92      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.93      0.90      0.91      1000\n",
      "           8       0.87      0.88      0.87      1000\n",
      "           9       0.86      0.93      0.89      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 19s 96us/sample - loss: 7.6053 - acc: 0.7969 - val_loss: 5.6360 - val_acc: 0.8211\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 20s 98us/sample - loss: 4.3910 - acc: 0.8280 - val_loss: 3.3957 - val_acc: 0.8275\n",
      "Test accuracy: 89.37% with regularizer:0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.91      1000\n",
      "           1       0.92      0.87      0.89      1000\n",
      "           2       0.90      0.92      0.91      1000\n",
      "           3       0.92      0.91      0.91      1000\n",
      "           4       0.91      0.85      0.88      1000\n",
      "           5       0.87      0.94      0.90      1000\n",
      "           6       0.92      0.89      0.90      1000\n",
      "           7       0.91      0.88      0.90      1000\n",
      "           8       0.85      0.86      0.85      1000\n",
      "           9       0.83      0.92      0.87      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.90      0.89      0.89     10000\n",
      "weighted avg       0.90      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 20s 102us/sample - loss: 1.6407 - acc: 0.7981 - val_loss: 1.4793 - val_acc: 0.8293\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 20s 98us/sample - loss: 1.4250 - acc: 0.8357 - val_loss: 1.3890 - val_acc: 0.8392\n",
      "Test accuracy: 90.80% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1000\n",
      "           1       0.92      0.89      0.90      1000\n",
      "           2       0.92      0.93      0.92      1000\n",
      "           3       0.92      0.93      0.92      1000\n",
      "           4       0.92      0.88      0.90      1000\n",
      "           5       0.90      0.94      0.92      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.92      0.89      0.91      1000\n",
      "           8       0.87      0.86      0.87      1000\n",
      "           9       0.87      0.94      0.90      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 21s 107us/sample - loss: 0.8338 - acc: 0.8003 - val_loss: 0.7018 - val_acc: 0.8293\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 20s 99us/sample - loss: 0.6684 - acc: 0.8368 - val_loss: 0.6560 - val_acc: 0.8411\n",
      "Test accuracy: 90.64% with regularizer:0.0001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1000\n",
      "           1       0.91      0.88      0.90      1000\n",
      "           2       0.93      0.93      0.93      1000\n",
      "           3       0.90      0.92      0.91      1000\n",
      "           4       0.92      0.89      0.90      1000\n",
      "           5       0.89      0.94      0.91      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.91      0.90      0.91      1000\n",
      "           8       0.87      0.87      0.87      1000\n",
      "           9       0.88      0.92      0.90      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 18s 92us/sample - loss: 17.0011 - acc: 0.1571 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 18s 89us/sample - loss: 2.3026 - acc: 0.0993 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Test accuracy: 10.00% with regularizer:1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1000\n",
      "           1       0.00      0.00      0.00      1000\n",
      "           2       0.00      0.00      0.00      1000\n",
      "           3       0.00      0.00      0.00      1000\n",
      "           4       0.10      1.00      0.18      1000\n",
      "           5       0.00      0.00      0.00      1000\n",
      "           6       0.00      0.00      0.00      1000\n",
      "           7       0.00      0.00      0.00      1000\n",
      "           8       0.00      0.00      0.00      1000\n",
      "           9       0.00      0.00      0.00      1000\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.01      0.10      0.02     10000\n",
      "weighted avg       0.01      0.10      0.02     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 1s 934us/sample - loss: 1.9120 - acc: 0.3740 - val_loss: 1.3868 - val_acc: 0.6379\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 1.2468 - acc: 0.6930 - val_loss: 1.1163 - val_acc: 0.7276\n",
      "Test accuracy: 79.80% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84      1000\n",
      "           1       0.88      0.75      0.81      1000\n",
      "           2       0.82      0.80      0.81      1000\n",
      "           3       0.83      0.83      0.83      1000\n",
      "           4       0.77      0.71      0.74      1000\n",
      "           5       0.68      0.91      0.78      1000\n",
      "           6       0.81      0.82      0.81      1000\n",
      "           7       0.90      0.75      0.82      1000\n",
      "           8       0.80      0.79      0.80      1000\n",
      "           9       0.68      0.83      0.75      1000\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.81      0.80      0.80     10000\n",
      "weighted avg       0.81      0.80      0.80     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_model, _ = train_mnist(X_train=train_dataset[:1000,:],y_train = train_labels[:1000,:],nn=False, regularizer=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 2.6493 - acc: 0.4890 - val_loss: 2.1322 - val_acc: 0.7096\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 1s 537us/sample - loss: 1.9840 - acc: 0.7360 - val_loss: 1.8774 - val_acc: 0.7553\n",
      "Test accuracy: 82.98% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80      1000\n",
      "           1       0.90      0.81      0.85      1000\n",
      "           2       0.79      0.91      0.84      1000\n",
      "           3       0.81      0.88      0.84      1000\n",
      "           4       0.89      0.76      0.82      1000\n",
      "           5       0.76      0.92      0.83      1000\n",
      "           6       0.91      0.78      0.84      1000\n",
      "           7       0.91      0.80      0.85      1000\n",
      "           8       0.78      0.84      0.81      1000\n",
      "           9       0.85      0.77      0.80      1000\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(X_train=train_dataset[:1000,:], y_train = train_labels[:1000,:],regularizer=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy is lower due to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 17s 83us/sample - loss: 0.7475 - acc: 0.7993 - val_loss: 0.6105 - val_acc: 0.8295\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 16s 78us/sample - loss: 0.5760 - acc: 0.8369 - val_loss: 0.5637 - val_acc: 0.8406\n",
      "Test accuracy: 90.85% with regularizer:0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1000\n",
      "           1       0.91      0.89      0.90      1000\n",
      "           2       0.91      0.94      0.93      1000\n",
      "           3       0.93      0.92      0.92      1000\n",
      "           4       0.93      0.88      0.90      1000\n",
      "           5       0.92      0.93      0.92      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.93      0.90      0.91      1000\n",
      "           8       0.87      0.88      0.87      1000\n",
      "           9       0.86      0.93      0.89      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 18s 89us/sample - loss: 0.5876 - acc: 0.8310 - val_loss: 0.4786 - val_acc: 0.8631\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 17s 85us/sample - loss: 0.4820 - acc: 0.8598 - val_loss: 0.4295 - val_acc: 0.8802\n",
      "Test accuracy: 93.73% with regularizer:0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      1000\n",
      "           1       0.94      0.93      0.93      1000\n",
      "           2       0.94      0.96      0.95      1000\n",
      "           3       0.93      0.95      0.94      1000\n",
      "           4       0.96      0.92      0.94      1000\n",
      "           5       0.96      0.95      0.95      1000\n",
      "           6       0.92      0.94      0.93      1000\n",
      "           7       0.95      0.94      0.94      1000\n",
      "           8       0.93      0.90      0.91      1000\n",
      "           9       0.91      0.95      0.93      1000\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 22s 109us/sample - loss: 1.2888 - acc: 0.8313 - val_loss: 1.0201 - val_acc: 0.8598\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 21s 104us/sample - loss: 0.9063 - acc: 0.8566 - val_loss: 0.7808 - val_acc: 0.8690\n",
      "Test accuracy: 92.87% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1000\n",
      "           1       0.92      0.93      0.92      1000\n",
      "           2       0.93      0.95      0.94      1000\n",
      "           3       0.93      0.93      0.93      1000\n",
      "           4       0.95      0.92      0.93      1000\n",
      "           5       0.93      0.95      0.94      1000\n",
      "           6       0.92      0.93      0.92      1000\n",
      "           7       0.97      0.91      0.94      1000\n",
      "           8       0.92      0.89      0.90      1000\n",
      "           9       0.87      0.95      0.91      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(dropout=True, regularizer=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout did a great job on avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 11s 55us/sample - loss: 0.5768 - acc: 0.8299 - val_loss: 0.4697 - val_acc: 0.8595\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 10s 50us/sample - loss: 0.4535 - acc: 0.8633 - val_loss: 0.4167 - val_acc: 0.8774\n",
      "Test accuracy: 93.33% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1000\n",
      "           1       0.94      0.92      0.93      1000\n",
      "           2       0.94      0.94      0.94      1000\n",
      "           3       0.93      0.95      0.94      1000\n",
      "           4       0.93      0.93      0.93      1000\n",
      "           5       0.93      0.95      0.94      1000\n",
      "           6       0.91      0.94      0.92      1000\n",
      "           7       0.95      0.93      0.94      1000\n",
      "           8       0.92      0.91      0.91      1000\n",
      "           9       0.94      0.93      0.93      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(multiple_nn=True, dropout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 12s 61us/sample - loss: 0.5645 - acc: 0.8308 - val_loss: 0.4580 - val_acc: 0.8623\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.4304 - acc: 0.8681 - val_loss: 0.4001 - val_acc: 0.8765\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.3883 - acc: 0.8813 - val_loss: 0.3762 - val_acc: 0.8860\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.3602 - acc: 0.8896 - val_loss: 0.3704 - val_acc: 0.8841\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.3406 - acc: 0.8947 - val_loss: 0.3525 - val_acc: 0.8917\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.3246 - acc: 0.8995 - val_loss: 0.3448 - val_acc: 0.8948\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.3113 - acc: 0.9031 - val_loss: 0.3480 - val_acc: 0.8944\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.2999 - acc: 0.9067 - val_loss: 0.3350 - val_acc: 0.8969\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.2896 - acc: 0.9094 - val_loss: 0.3309 - val_acc: 0.8988\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 11s 56us/sample - loss: 0.2809 - acc: 0.9119 - val_loss: 0.3272 - val_acc: 0.9009\n",
      "Test accuracy: 95.34% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96      1000\n",
      "           1       0.96      0.94      0.95      1000\n",
      "           2       0.96      0.96      0.96      1000\n",
      "           3       0.95      0.96      0.96      1000\n",
      "           4       0.95      0.95      0.95      1000\n",
      "           5       0.97      0.95      0.96      1000\n",
      "           6       0.93      0.95      0.94      1000\n",
      "           7       0.98      0.94      0.96      1000\n",
      "           8       0.94      0.93      0.93      1000\n",
      "           9       0.94      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(multiple_nn=True, dropout=True, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "good job. we have time.\n",
    "\n",
    "now try decay learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = .5\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                        decay_steps = 10000,\n",
    "                                                        decay_rate = .96,\n",
    "                                                        staircase = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "200000/200000 [==============================] - 13s 65us/sample - loss: 0.5070 - acc: 0.8439 - val_loss: 0.4151 - val_acc: 0.8729\n",
      "Epoch 2/10\n",
      "200000/200000 [==============================] - 12s 58us/sample - loss: 0.3918 - acc: 0.8790 - val_loss: 0.3961 - val_acc: 0.8812\n",
      "Epoch 3/10\n",
      "200000/200000 [==============================] - 12s 58us/sample - loss: 0.3559 - acc: 0.8899 - val_loss: 0.3627 - val_acc: 0.8873\n",
      "Epoch 4/10\n",
      "200000/200000 [==============================] - 12s 58us/sample - loss: 0.3333 - acc: 0.8958 - val_loss: 0.3438 - val_acc: 0.8949\n",
      "Epoch 5/10\n",
      "200000/200000 [==============================] - 11s 57us/sample - loss: 0.3168 - acc: 0.9010 - val_loss: 0.3456 - val_acc: 0.8939\n",
      "Epoch 6/10\n",
      "200000/200000 [==============================] - 11s 57us/sample - loss: 0.3048 - acc: 0.9042 - val_loss: 0.3461 - val_acc: 0.8954\n",
      "Epoch 7/10\n",
      "200000/200000 [==============================] - 11s 57us/sample - loss: 0.2925 - acc: 0.9079 - val_loss: 0.3377 - val_acc: 0.8966\n",
      "Epoch 8/10\n",
      "200000/200000 [==============================] - 12s 59us/sample - loss: 0.2818 - acc: 0.9109 - val_loss: 0.3402 - val_acc: 0.8968\n",
      "Epoch 9/10\n",
      "200000/200000 [==============================] - 12s 58us/sample - loss: 0.2748 - acc: 0.9133 - val_loss: 0.3380 - val_acc: 0.8988\n",
      "Epoch 10/10\n",
      "200000/200000 [==============================] - 12s 58us/sample - loss: 0.2680 - acc: 0.9149 - val_loss: 0.3419 - val_acc: 0.8995\n",
      "Test accuracy: 95.19% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1000\n",
      "           1       0.96      0.94      0.95      1000\n",
      "           2       0.96      0.97      0.96      1000\n",
      "           3       0.94      0.97      0.95      1000\n",
      "           4       0.95      0.95      0.95      1000\n",
      "           5       0.96      0.96      0.96      1000\n",
      "           6       0.96      0.95      0.95      1000\n",
      "           7       0.98      0.94      0.96      1000\n",
      "           8       0.93      0.92      0.92      1000\n",
      "           9       0.94      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(multiple_nn=True, dropout=True, epochs=10, learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "didn't change much.\n",
    "\n",
    "now try more epochs with callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping's monitor should use validation_data's params, like val_loss or val_acc.\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=2)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('checkpoint/nn_model.ckpt',\n",
    "                                            monitor='val_acc',\n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            load_weights_on_restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.2192 - acc: 0.9306\n",
      "Epoch 00001: val_acc did not improve from 0.90270\n",
      "200000/200000 [==============================] - 14s 69us/sample - loss: 0.2192 - acc: 0.9306 - val_loss: 0.4390 - val_acc: 0.9010\n",
      "Epoch 2/100\n",
      "199296/200000 [============================>.] - ETA: 0s - loss: 0.2215 - acc: 0.9296\n",
      "Epoch 00002: val_acc did not improve from 0.90270\n",
      "200000/200000 [==============================] - 12s 61us/sample - loss: 0.2217 - acc: 0.9295 - val_loss: 0.4183 - val_acc: 0.9015\n",
      "Epoch 3/100\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 0.2172 - acc: 0.9310\n",
      "Epoch 00003: val_acc did not improve from 0.90270\n",
      "200000/200000 [==============================] - 12s 61us/sample - loss: 0.2172 - acc: 0.9310 - val_loss: 0.4455 - val_acc: 0.9001\n",
      "Epoch 4/100\n",
      "199168/200000 [============================>.] - ETA: 0s - loss: 0.2198 - acc: 0.9303\n",
      "Epoch 00004: val_acc did not improve from 0.90270\n",
      "200000/200000 [==============================] - 12s 62us/sample - loss: 0.2198 - acc: 0.9303 - val_loss: 0.4615 - val_acc: 0.8989\n",
      "Test accuracy: 95.44% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1000\n",
      "           1       0.97      0.95      0.96      1000\n",
      "           2       0.96      0.96      0.96      1000\n",
      "           3       0.95      0.96      0.96      1000\n",
      "           4       0.96      0.95      0.96      1000\n",
      "           5       0.98      0.95      0.97      1000\n",
      "           6       0.95      0.95      0.95      1000\n",
      "           7       0.98      0.94      0.96      1000\n",
      "           8       0.91      0.95      0.93      1000\n",
      "           9       0.95      0.95      0.95      1000\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model, _ = train_mnist(multiple_nn=True, dropout=True, epochs=100, \n",
    "                          learning_rate=lr_schedule, \n",
    "                          callback=[callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looks like 95% is the best?\n",
    "\n",
    "try another optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      " 13824/200000 [=>............................] - ETA: 28s - loss: nan - acc: 0.1427"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-252-57ee9e988a3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, train_labels, validation_data=(valid_dataset, valid_labels), callbacks=[callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_96 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 202,274\n",
      "Trainable params: 202,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.save('test.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further training on existing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.load_model('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_allclose(model.predict(train_dataset),\n",
    "                          new_model.predict(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_96 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 202,274\n",
      "Trainable params: 202,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 97us/sample - loss: 0.2258 - acc: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22576687259974715, 0.9542]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
