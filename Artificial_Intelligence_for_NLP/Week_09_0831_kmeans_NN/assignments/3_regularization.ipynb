{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use high-level API with keras instead of low-level API with too many statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_print(y_true, y_pred):\n",
    "    if len(np.shape(y_true)) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(np.shape(y_pred)) > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(X_train=train_dataset, y_train=train_labels, \n",
    "                X_test=test_dataset, y_test=test_labels,\n",
    "                learning_rate = .1,\n",
    "                dropout = False,\n",
    "                dropout_rate = .5,\n",
    "                nn=True,\n",
    "                multiple_nn = False,\n",
    "                regularizer=0., \n",
    "                epochs=2, \n",
    "                batch_size=128,\n",
    "                optimizer=keras.optimizers.SGD,\n",
    "                callback = [],\n",
    "                validation_data=(valid_dataset, valid_labels)):\n",
    "    reset_graph()\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Flatten(input_shape=(784,)))\n",
    "    if nn:\n",
    "        if multiple_nn:\n",
    "            model.add(keras.layers.Dense(100, activation=tf.nn.relu,\n",
    "                                        kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "            model.add(keras.layers.Dense(100, activation=tf.nn.relu,\n",
    "                                        kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "        model.add(keras.layers.Dense(1024, activation=tf.nn.relu, \n",
    "                                     kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "        if dropout:\n",
    "            model.add(keras.layers.Dropout(dropout_rate))\n",
    "    model.add(keras.layers.Dense(10, activation=tf.nn.softmax, \n",
    "                                 kernel_regularizer=keras.regularizers.l2(regularizer)))\n",
    "    \n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=callback)\n",
    "    print(f'Test accuracy: {model.evaluate(X_test, y_test, verbose=0)[1]*100:.2f}% with regularizer: {regularizer}')\n",
    "    classification_print(y_test, model.predict(X_test))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0906 23:46:25.837602  5604 deprecation.py:506] From C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 2s 12us/sample - loss: 0.6869 - acc: 0.8184 - val_loss: 0.6541 - val_acc: 0.8289\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 2s 11us/sample - loss: 0.6409 - acc: 0.8301 - val_loss: 0.6506 - val_acc: 0.8284\n",
      "Test accuracy: 89.68% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91      1000\n",
      "           1       0.91      0.89      0.90      1000\n",
      "           2       0.91      0.93      0.92      1000\n",
      "           3       0.93      0.92      0.92      1000\n",
      "           4       0.92      0.86      0.89      1000\n",
      "           5       0.90      0.93      0.91      1000\n",
      "           6       0.87      0.90      0.88      1000\n",
      "           7       0.88      0.90      0.89      1000\n",
      "           8       0.86      0.86      0.86      1000\n",
      "           9       0.86      0.90      0.88      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_91 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logistic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 4s 21us/sample - loss: 0.8282 - acc: 0.8146 - val_loss: 0.7857 - val_acc: 0.8202\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 3s 17us/sample - loss: 0.7795 - acc: 0.8214 - val_loss: 0.7874 - val_acc: 0.8186\n",
      "Test accuracy: 88.85% with regularizer:0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91      1000\n",
      "           1       0.93      0.87      0.90      1000\n",
      "           2       0.91      0.93      0.92      1000\n",
      "           3       0.91      0.91      0.91      1000\n",
      "           4       0.91      0.86      0.89      1000\n",
      "           5       0.91      0.91      0.91      1000\n",
      "           6       0.90      0.89      0.90      1000\n",
      "           7       0.92      0.87      0.89      1000\n",
      "           8       0.80      0.86      0.83      1000\n",
      "           9       0.80      0.89      0.85      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False, regularizer=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 4s 18us/sample - loss: 0.7152 - acc: 0.8181 - val_loss: 0.6869 - val_acc: 0.8262\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 3s 15us/sample - loss: 0.6696 - acc: 0.8290 - val_loss: 0.6739 - val_acc: 0.8304\n",
      "Test accuracy: 89.71% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91      1000\n",
      "           1       0.92      0.89      0.90      1000\n",
      "           2       0.89      0.93      0.91      1000\n",
      "           3       0.91      0.93      0.92      1000\n",
      "           4       0.90      0.87      0.88      1000\n",
      "           5       0.92      0.92      0.92      1000\n",
      "           6       0.91      0.89      0.90      1000\n",
      "           7       0.90      0.89      0.90      1000\n",
      "           8       0.84      0.85      0.85      1000\n",
      "           9       0.86      0.90      0.88      1000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False, regularizer=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 4s 19us/sample - loss: 1.9211 - acc: 0.7070 - val_loss: 1.9044 - val_acc: 0.6929\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 3s 15us/sample - loss: 1.8839 - acc: 0.7074 - val_loss: 1.8995 - val_acc: 0.6621\n",
      "Test accuracy: 72.71% with regularizer:1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.68      0.79      1000\n",
      "           1       0.79      0.74      0.76      1000\n",
      "           2       0.55      0.89      0.68      1000\n",
      "           3       0.89      0.80      0.84      1000\n",
      "           4       0.87      0.66      0.75      1000\n",
      "           5       0.93      0.76      0.84      1000\n",
      "           6       0.96      0.36      0.52      1000\n",
      "           7       0.92      0.66      0.77      1000\n",
      "           8       0.69      0.83      0.75      1000\n",
      "           9       0.45      0.89      0.60      1000\n",
      "\n",
      "    accuracy                           0.73     10000\n",
      "   macro avg       0.80      0.73      0.73     10000\n",
      "weighted avg       0.80      0.73      0.73     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_model, logisitc_history = train_mnist(nn=False, regularizer=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 17s 83us/sample - loss: 0.7475 - acc: 0.7993 - val_loss: 0.6105 - val_acc: 0.8295\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 16s 78us/sample - loss: 0.5760 - acc: 0.8369 - val_loss: 0.5637 - val_acc: 0.8406\n",
      "Test accuracy: 90.85% with regularizer:0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1000\n",
      "           1       0.91      0.89      0.90      1000\n",
      "           2       0.91      0.94      0.93      1000\n",
      "           3       0.93      0.92      0.92      1000\n",
      "           4       0.93      0.88      0.90      1000\n",
      "           5       0.92      0.93      0.92      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.93      0.90      0.91      1000\n",
      "           8       0.87      0.88      0.87      1000\n",
      "           9       0.86      0.93      0.89      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 19s 96us/sample - loss: 7.6053 - acc: 0.7969 - val_loss: 5.6360 - val_acc: 0.8211\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 20s 98us/sample - loss: 4.3910 - acc: 0.8280 - val_loss: 3.3957 - val_acc: 0.8275\n",
      "Test accuracy: 89.37% with regularizer:0.01\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.91      1000\n",
      "           1       0.92      0.87      0.89      1000\n",
      "           2       0.90      0.92      0.91      1000\n",
      "           3       0.92      0.91      0.91      1000\n",
      "           4       0.91      0.85      0.88      1000\n",
      "           5       0.87      0.94      0.90      1000\n",
      "           6       0.92      0.89      0.90      1000\n",
      "           7       0.91      0.88      0.90      1000\n",
      "           8       0.85      0.86      0.85      1000\n",
      "           9       0.83      0.92      0.87      1000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.90      0.89      0.89     10000\n",
      "weighted avg       0.90      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 20s 102us/sample - loss: 1.6407 - acc: 0.7981 - val_loss: 1.4793 - val_acc: 0.8293\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 20s 98us/sample - loss: 1.4250 - acc: 0.8357 - val_loss: 1.3890 - val_acc: 0.8392\n",
      "Test accuracy: 90.80% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1000\n",
      "           1       0.92      0.89      0.90      1000\n",
      "           2       0.92      0.93      0.92      1000\n",
      "           3       0.92      0.93      0.92      1000\n",
      "           4       0.92      0.88      0.90      1000\n",
      "           5       0.90      0.94      0.92      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.92      0.89      0.91      1000\n",
      "           8       0.87      0.86      0.87      1000\n",
      "           9       0.87      0.94      0.90      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 21s 107us/sample - loss: 0.8338 - acc: 0.8003 - val_loss: 0.7018 - val_acc: 0.8293\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 20s 99us/sample - loss: 0.6684 - acc: 0.8368 - val_loss: 0.6560 - val_acc: 0.8411\n",
      "Test accuracy: 90.64% with regularizer:0.0001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92      1000\n",
      "           1       0.91      0.88      0.90      1000\n",
      "           2       0.93      0.93      0.93      1000\n",
      "           3       0.90      0.92      0.91      1000\n",
      "           4       0.92      0.89      0.90      1000\n",
      "           5       0.89      0.94      0.91      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.91      0.90      0.91      1000\n",
      "           8       0.87      0.87      0.87      1000\n",
      "           9       0.88      0.92      0.90      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 18s 92us/sample - loss: 17.0011 - acc: 0.1571 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 18s 89us/sample - loss: 2.3026 - acc: 0.0993 - val_loss: 2.3026 - val_acc: 0.1000\n",
      "Test accuracy: 10.00% with regularizer:1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1000\n",
      "           1       0.00      0.00      0.00      1000\n",
      "           2       0.00      0.00      0.00      1000\n",
      "           3       0.00      0.00      0.00      1000\n",
      "           4       0.10      1.00      0.18      1000\n",
      "           5       0.00      0.00      0.00      1000\n",
      "           6       0.00      0.00      0.00      1000\n",
      "           7       0.00      0.00      0.00      1000\n",
      "           8       0.00      0.00      0.00      1000\n",
      "           9       0.00      0.00      0.00      1000\n",
      "\n",
      "    accuracy                           0.10     10000\n",
      "   macro avg       0.01      0.10      0.02     10000\n",
      "weighted avg       0.01      0.10      0.02     10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist(regularizer=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 1s 934us/sample - loss: 1.9120 - acc: 0.3740 - val_loss: 1.3868 - val_acc: 0.6379\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 0s 133us/sample - loss: 1.2468 - acc: 0.6930 - val_loss: 1.1163 - val_acc: 0.7276\n",
      "Test accuracy: 79.80% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84      1000\n",
      "           1       0.88      0.75      0.81      1000\n",
      "           2       0.82      0.80      0.81      1000\n",
      "           3       0.83      0.83      0.83      1000\n",
      "           4       0.77      0.71      0.74      1000\n",
      "           5       0.68      0.91      0.78      1000\n",
      "           6       0.81      0.82      0.81      1000\n",
      "           7       0.90      0.75      0.82      1000\n",
      "           8       0.80      0.79      0.80      1000\n",
      "           9       0.68      0.83      0.75      1000\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.81      0.80      0.80     10000\n",
      "weighted avg       0.81      0.80      0.80     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_model, _ = train_mnist(X_train=train_dataset[:1000,:],y_train = train_labels[:1000,:],nn=False, regularizer=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 2.6493 - acc: 0.4890 - val_loss: 2.1322 - val_acc: 0.7096\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 1s 537us/sample - loss: 1.9840 - acc: 0.7360 - val_loss: 1.8774 - val_acc: 0.7553\n",
      "Test accuracy: 82.98% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80      1000\n",
      "           1       0.90      0.81      0.85      1000\n",
      "           2       0.79      0.91      0.84      1000\n",
      "           3       0.81      0.88      0.84      1000\n",
      "           4       0.89      0.76      0.82      1000\n",
      "           5       0.76      0.92      0.83      1000\n",
      "           6       0.91      0.78      0.84      1000\n",
      "           7       0.91      0.80      0.85      1000\n",
      "           8       0.78      0.84      0.81      1000\n",
      "           9       0.85      0.77      0.80      1000\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.84      0.83      0.83     10000\n",
      "weighted avg       0.84      0.83      0.83     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(X_train=train_dataset[:1000,:], y_train = train_labels[:1000,:],regularizer=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy is lower due to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 17s 83us/sample - loss: 0.7475 - acc: 0.7993 - val_loss: 0.6105 - val_acc: 0.8295\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 16s 78us/sample - loss: 0.5760 - acc: 0.8369 - val_loss: 0.5637 - val_acc: 0.8406\n",
      "Test accuracy: 90.85% with regularizer:0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92      1000\n",
      "           1       0.91      0.89      0.90      1000\n",
      "           2       0.91      0.94      0.93      1000\n",
      "           3       0.93      0.92      0.92      1000\n",
      "           4       0.93      0.88      0.90      1000\n",
      "           5       0.92      0.93      0.92      1000\n",
      "           6       0.91      0.91      0.91      1000\n",
      "           7       0.93      0.90      0.91      1000\n",
      "           8       0.87      0.88      0.87      1000\n",
      "           9       0.86      0.93      0.89      1000\n",
      "\n",
      "    accuracy                           0.91     10000\n",
      "   macro avg       0.91      0.91      0.91     10000\n",
      "weighted avg       0.91      0.91      0.91     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, nn_history = train_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 18s 89us/sample - loss: 0.5876 - acc: 0.8310 - val_loss: 0.4786 - val_acc: 0.8631\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 17s 85us/sample - loss: 0.4820 - acc: 0.8598 - val_loss: 0.4295 - val_acc: 0.8802\n",
      "Test accuracy: 93.73% with regularizer:0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      1000\n",
      "           1       0.94      0.93      0.93      1000\n",
      "           2       0.94      0.96      0.95      1000\n",
      "           3       0.93      0.95      0.94      1000\n",
      "           4       0.96      0.92      0.94      1000\n",
      "           5       0.96      0.95      0.95      1000\n",
      "           6       0.92      0.94      0.93      1000\n",
      "           7       0.95      0.94      0.94      1000\n",
      "           8       0.93      0.90      0.91      1000\n",
      "           9       0.91      0.95      0.93      1000\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.94      0.94      0.94     10000\n",
      "weighted avg       0.94      0.94      0.94     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "200000/200000 [==============================] - 22s 109us/sample - loss: 1.2888 - acc: 0.8313 - val_loss: 1.0201 - val_acc: 0.8598\n",
      "Epoch 2/2\n",
      "200000/200000 [==============================] - 21s 104us/sample - loss: 0.9063 - acc: 0.8566 - val_loss: 0.7808 - val_acc: 0.8690\n",
      "Test accuracy: 92.87% with regularizer:0.001\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95      1000\n",
      "           1       0.92      0.93      0.92      1000\n",
      "           2       0.93      0.95      0.94      1000\n",
      "           3       0.93      0.93      0.93      1000\n",
      "           4       0.95      0.92      0.93      1000\n",
      "           5       0.93      0.95      0.94      1000\n",
      "           6       0.92      0.93      0.92      1000\n",
      "           7       0.97      0.91      0.94      1000\n",
      "           8       0.92      0.89      0.90      1000\n",
      "           9       0.87      0.95      0.91      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(dropout=True, regularizer=.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout did a great job on avoiding overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping's monitor should use validation_data's params, like val_loss or val_acc.\n",
    "stop_callback = keras.callbacks.EarlyStopping(monitor='val_acc', mode='max', patience=2)\n",
    "checkpoint_path='checkpoint/nn_model.ckpt'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                            monitor='val_acc',\n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            load_weights_on_restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "199168/200000 [============================>.] - ETA: 0s - loss: 0.1163 - acc: 0.9599\n",
      "Epoch 00001: val_acc improved from -inf to 0.91080, saving model to checkpoint/nn_model.ckpt\n",
      "200000/200000 [==============================] - 12s 62us/sample - loss: 0.1162 - acc: 0.9599 - val_loss: 0.5095 - val_acc: 0.9108\n",
      "Epoch 2/2\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9614\n",
      "Epoch 00002: val_acc improved from 0.91080 to 0.91140, saving model to checkpoint/nn_model.ckpt\n",
      "200000/200000 [==============================] - 12s 61us/sample - loss: 0.1119 - acc: 0.9614 - val_loss: 0.5149 - val_acc: 0.9114\n",
      "Test accuracy: 95.98% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1000\n",
      "           1       0.95      0.96      0.96      1000\n",
      "           2       0.96      0.96      0.96      1000\n",
      "           3       0.97      0.97      0.97      1000\n",
      "           4       0.96      0.95      0.96      1000\n",
      "           5       0.97      0.96      0.97      1000\n",
      "           6       0.95      0.96      0.95      1000\n",
      "           7       0.97      0.95      0.96      1000\n",
      "           8       0.94      0.95      0.94      1000\n",
      "           9       0.96      0.97      0.96      1000\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(multiple_nn=True, dropout=True, callback=[stop_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try decay learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "199424/200000 [============================>.] - ETA: 0s - loss: 0.1030 - acc: 0.9647\n",
      "Epoch 00001: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 62us/sample - loss: 0.1030 - acc: 0.9647 - val_loss: 0.5373 - val_acc: 0.9100\n",
      "Epoch 2/10\n",
      "199936/200000 [============================>.] - ETA: 0s - loss: 0.0995 - acc: 0.9659\n",
      "Epoch 00002: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 62us/sample - loss: 0.0995 - acc: 0.9660 - val_loss: 0.5499 - val_acc: 0.9112\n",
      "Epoch 3/10\n",
      "199552/200000 [============================>.] - ETA: 0s - loss: 0.1046 - acc: 0.9635\n",
      "Epoch 00003: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 61us/sample - loss: 0.1046 - acc: 0.9635 - val_loss: 0.5527 - val_acc: 0.9103\n",
      "Epoch 4/10\n",
      "199168/200000 [============================>.] - ETA: 0s - loss: 0.1012 - acc: 0.9651\n",
      "Epoch 00004: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 61us/sample - loss: 0.1011 - acc: 0.9652 - val_loss: 0.5683 - val_acc: 0.9109\n",
      "Test accuracy: 96.01% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1000\n",
      "           1       0.96      0.96      0.96      1000\n",
      "           2       0.96      0.97      0.96      1000\n",
      "           3       0.98      0.97      0.97      1000\n",
      "           4       0.95      0.95      0.95      1000\n",
      "           5       0.98      0.96      0.97      1000\n",
      "           6       0.94      0.95      0.95      1000\n",
      "           7       0.97      0.96      0.96      1000\n",
      "           8       0.94      0.95      0.95      1000\n",
      "           9       0.96      0.96      0.96      1000\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate = .1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                        decay_steps = 10000,\n",
    "                                                        decay_rate = .96,\n",
    "                                                        staircase = True)\n",
    "nn_model, _ = train_mnist(multiple_nn=True, dropout=True, \n",
    "                          epochs=10, learning_rate=lr_schedule,\n",
    "                         callback=[stop_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9664\n",
      "Epoch 00001: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 62us/sample - loss: 0.0987 - acc: 0.9664 - val_loss: 0.5307 - val_acc: 0.9092\n",
      "Epoch 2/10\n",
      "199680/200000 [============================>.] - ETA: 0s - loss: 0.0940 - acc: 0.9680\n",
      "Epoch 00002: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 60us/sample - loss: 0.0940 - acc: 0.9680 - val_loss: 0.5422 - val_acc: 0.9114\n",
      "Epoch 3/10\n",
      "199296/200000 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9681\n",
      "Epoch 00003: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 60us/sample - loss: 0.0933 - acc: 0.9680 - val_loss: 0.5618 - val_acc: 0.9092\n",
      "Epoch 4/10\n",
      "199424/200000 [============================>.] - ETA: 0s - loss: 0.0898 - acc: 0.9697\n",
      "Epoch 00004: val_acc did not improve from 0.91140\n",
      "200000/200000 [==============================] - 12s 59us/sample - loss: 0.0898 - acc: 0.9697 - val_loss: 0.5740 - val_acc: 0.9110\n",
      "Test accuracy: 95.89% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1000\n",
      "           1       0.95      0.96      0.95      1000\n",
      "           2       0.96      0.97      0.96      1000\n",
      "           3       0.98      0.97      0.97      1000\n",
      "           4       0.95      0.95      0.95      1000\n",
      "           5       0.97      0.96      0.97      1000\n",
      "           6       0.94      0.95      0.95      1000\n",
      "           7       0.97      0.95      0.96      1000\n",
      "           8       0.95      0.95      0.95      1000\n",
      "           9       0.96      0.96      0.96      1000\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(multiple_nn=True, dropout=True, dropout_rate=.3, epochs=10,learning_rate=lr_schedule,\n",
    "                         callback=[stop_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "199648/200000 [============================>.] - ETA: 0s - loss: 0.1546 - acc: 0.9492\n",
      "Epoch 00001: val_acc did not improve from 0.91150\n",
      "200000/200000 [==============================] - 27s 137us/sample - loss: 0.1546 - acc: 0.9492 - val_loss: 0.4004 - val_acc: 0.9069\n",
      "Epoch 2/10\n",
      "199616/200000 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9487\n",
      "Epoch 00002: val_acc did not improve from 0.91150\n",
      "200000/200000 [==============================] - 27s 134us/sample - loss: 0.1534 - acc: 0.9488 - val_loss: 0.4086 - val_acc: 0.9087\n",
      "Epoch 3/10\n",
      "199968/200000 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.9504\n",
      "Epoch 00003: val_acc did not improve from 0.91150\n",
      "200000/200000 [==============================] - 27s 136us/sample - loss: 0.1489 - acc: 0.9504 - val_loss: 0.4230 - val_acc: 0.9077\n",
      "Epoch 4/10\n",
      "199904/200000 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.9518\n",
      "Epoch 00004: val_acc did not improve from 0.91150\n",
      "200000/200000 [==============================] - 27s 134us/sample - loss: 0.1430 - acc: 0.9518 - val_loss: 0.4329 - val_acc: 0.9083\n",
      "Test accuracy: 95.74% with regularizer: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96      1000\n",
      "           1       0.96      0.96      0.96      1000\n",
      "           2       0.96      0.97      0.96      1000\n",
      "           3       0.97      0.96      0.97      1000\n",
      "           4       0.96      0.95      0.95      1000\n",
      "           5       0.97      0.96      0.97      1000\n",
      "           6       0.94      0.96      0.95      1000\n",
      "           7       0.97      0.95      0.96      1000\n",
      "           8       0.94      0.94      0.94      1000\n",
      "           9       0.94      0.96      0.95      1000\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.96      0.96      0.96     10000\n",
      "weighted avg       0.96      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn_model, _ = train_mnist(multiple_nn=True, dropout=True, dropout_rate=.3, epochs=10,learning_rate=lr_schedule,\n",
    "                          batch_size=32,\n",
    "                         callback=[stop_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try a whole new structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "checkpoint_path = 'checkpoint/nn_model_final.ckpt'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                            monitor='val_acc',\n",
    "                                            verbose=1,\n",
    "                                            save_best_only=True,\n",
    "                                            load_weights_on_restart=True)\n",
    "model_final = keras.models.Sequential([\n",
    "    keras.layers.Dense(4*784, input_shape=(784,), activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(2*784, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(784, activation=tf.nn.relu),\n",
    "    keras.layers.Dropout(.5),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "model_final.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "model_final.fit(train_dataset, train_labels, batch_size=1024,\n",
    "                validation_data=(valid_dataset, valid_labels),\n",
    "                epochs=1000,\n",
    "                callbacks=[stop_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 4s 439us/sample - loss: 0.1437 - acc: 0.9588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14367311419695616, 0.9588]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.evaluate(test_dataset, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 hour trainging with complicated structure seems not that powerful as expected.\n",
    "\n",
    "maybe more training data is needed.\n",
    "\n",
    "datasets from example is 200k, while notMNIST dataset got 520k training data, higher accuracy need more data, perhaps?\n",
    "\n",
    "implemented on service or with GPU is an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use CNN will have better result.implement it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = r\"D:\\Github\\NLP\\Artificial_Intelligence_for_NLP\\Week_09_0831_kmeans_NN\\assignments\\notMNIST_large\"\n",
    "test_file = r\"D:\\Github\\NLP\\Artificial_Intelligence_for_NLP\\Week_09_0831_kmeans_NN\\assignments\\notMNIST_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = np.zeros((1,28,28))\n",
    "summary_labels = np.zeros(1)\n",
    "\n",
    "test_summary = np.zeros((1, 28, 28))\n",
    "test_summary_labels = np.zeros(1)\n",
    "\n",
    "for p in os.listdir(train_file):\n",
    "    pickle_path = os.path.join(train_file, p)\n",
    "    if os.path.isfile(pickle_path):\n",
    "        index = 'ABCDEFGHIJ'.index(os.path.splitext(pickle_path)[0][-1])\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            temp = pickle.load(f)\n",
    "            size = temp.shape[0]\n",
    "            summary = np.r_[summary, temp]\n",
    "            summary_labels = np.r_[summary_labels, np.full((size,),index)]\n",
    "\n",
    "\n",
    "for p in os.listdir(test_file):\n",
    "    pickle_path = os.path.join(test_file, p)\n",
    "    if os.path.isfile(pickle_path):\n",
    "        index = 'ABCDEFGHIJ'.index(os.path.splitext(pickle_path)[0][-1])\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            temp = pickle.load(f)\n",
    "            size = temp.shape[0]\n",
    "            test_summary = np.r_[test_summary, temp]\n",
    "            test_summary_labels = np.r_[test_summary_labels, np.full((size,),index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allMNIST.pickle', 'wb') as f:\n",
    "    save = {\n",
    "        'train_dataset': summary[1:],\n",
    "        'train_labels': summary_labels[1:],\n",
    "        'test_dataset': test_summary[1:],\n",
    "        'test_labels': test_summary_labels[1:]\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allMNIST.pickle', 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save\n",
    "\n",
    "\n",
    "def shuffle(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation, :, :]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = shuffle(train_dataset, train_labels)\n",
    "test_dataset, test_labels = shuffle(test_dataset, test_labels)\n",
    "encoder = OneHotEncoder(categories='auto')\n",
    "train_labels_1hot = encoder.fit_transform(train_labels.reshape(-1,1)).toarray()\n",
    "test_labels_1hot = encoder.fit_transform(test_labels.reshape(-1,1)).toarray()\n",
    "train_flatten = train_dataset.reshape(-1, 784)\n",
    "test_flatten = test_dataset.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((529114, 28, 28), (529114,), (18724, 28, 28), (18724,))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.shape, train_labels.shape, test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's train model on service with more CPUs computation resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir(prefix=''):\n",
    "    now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    root_logdir = 'tf_logs'\n",
    "    if prefix:\n",
    "        prefix += '-'\n",
    "    name = prefix + 'run_' + now\n",
    "    return os.path.join(root_logdir, name)\n",
    "\n",
    "\n",
    "logdir = log_dir('high_level_API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    reset_graph()\n",
    "    checkpoint_path = 'tmp/high_level_API_complicate_CNN'\n",
    "    units = 28 * 28\n",
    "    initial_learning_rate = .1\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                                 decay_steps=10000,\n",
    "                                                                 decay_rate=.96,\n",
    "                                                                 staircase=True)\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                     monitor='val_acc',\n",
    "                                                     verbose=1,\n",
    "                                                     save_best_only=True,\n",
    "                                                     load_weights_on_restart=True)\n",
    "    stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                                     mode='max',\n",
    "                                                     patience=10)\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger('csv_logger')\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(4 * units, activation=tf.nn.relu, input_shape=(784, )),\n",
    "        tf.keras.layers.Dropout(.5),\n",
    "        tf.keras.layers.Dense(2 * units, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(.5),\n",
    "        tf.keras.layers.Dense(units, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(.5),\n",
    "        tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr_schedule),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_flatten, train_labels_1hot, validation_split=.2,\n",
    "              epochs=100, batch_size=1024, callbacks=[cp_callback,\n",
    "                                     stop_callback,\n",
    "                                     csv_logger,\n",
    "                                     tensorboard])\n",
    "\n",
    "    print(\"Model's performance on test dataset:\")\n",
    "    print(f\"Test accuracy: {model.evaluate(test_flatten, test_labels_1hot)[1] * 100:.3f}%\")\n",
    "    model.save('model.h5')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get trained model from service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0908 00:04:42.193841  6940 deprecation.py:506] From C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0908 00:04:42.196840  6940 deprecation.py:506] From C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18724/18724 [==============================] - 7s 399us/sample - loss: 0.0904 - acc: 0.9767s - loss: 0.0885 - acc: 0 - ETA: 0s - loss: 0.0901 - a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09038804665404582, 0.97666097]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_flatten, test_labels_1hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dingding! got test accuracy with `97.67%`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('csv_logger_1')\n",
    "df2 = pd.read_csv('csv_logger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.848216</td>\n",
       "      <td>0.498431</td>\n",
       "      <td>0.886376</td>\n",
       "      <td>0.371921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.877139</td>\n",
       "      <td>0.402231</td>\n",
       "      <td>0.895146</td>\n",
       "      <td>0.341904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.886716</td>\n",
       "      <td>0.368978</td>\n",
       "      <td>0.900523</td>\n",
       "      <td>0.320959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.893610</td>\n",
       "      <td>0.344003</td>\n",
       "      <td>0.907572</td>\n",
       "      <td>0.302903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.898739</td>\n",
       "      <td>0.327150</td>\n",
       "      <td>0.910152</td>\n",
       "      <td>0.295743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch       acc      loss   val_acc  val_loss\n",
       "0      0  0.848216  0.498431  0.886376  0.371921\n",
       "1      1  0.877139  0.402231  0.895146  0.341904\n",
       "2      2  0.886716  0.368978  0.900523  0.320959\n",
       "3      3  0.893610  0.344003  0.907572  0.302903\n",
       "4      4  0.898739  0.327150  0.910152  0.295743"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.plot(x='epoch', y=['acc', 'loss', 'val_acc', 'val_loss'], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after change batch_size from 32 to 1024 and training continue from checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEJCAYAAACaFuz/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de5QU5bnv8e8zwwgqiCAXFYiAEoxyVTReEpiERDDHQGI8EaNGPS5dxqiJ2XF7i8atMdmJ2fEkK0TD2seoCQmwvZxwtrfEyEAwahCDckfEIKOogICMisD0c/7onpnqe89M95Tzzu+z1qyueuutt593uvqpt6qrq83dERGRcFXFHYCIiFSWEr2ISOCU6EVEAqdELyISOCV6EZHAKdGLiASuaKI3s3vM7G0zW5FnuZnZL8xsvZm9ZGbHlT9MERFpq1JG9PcCUwssPx0Ykfq7FLir/WGJiEi5dCtWwd0XmdnQAlWmA/d78ptXz5rZwWZ2mLtvLtTuwQcf7EcddVSrgu1M3nvvPQ488MC4w6iYkPsXct9A/evsli5dutXd+7dmnaKJvgSDgE2R+fpUWcFEP3DgQJ5//vkyPP1HU11dHbW1tXGHUTEh9y/kvoH619mZ2cbWrlOORG85ynLeV8HMLiV5eof+/ftTV1dXhqf/aGpoaFD/OqmQ+wbqX1dUjkRfDwyJzA8G3shV0d1nAbMARo4c6SHvdUMfVYTcv5D7BupfV1SOyyvnA19PXX1zErCz2Pl5ERHpOEVH9Gb2B6AW6Gdm9cD3gRoAd78beBT4ArAeeB+4qFLBiohI65Vy1c05RZY78M2yRSQiImWlb8aKiAROiV5EJHDluOqmTXY27uRXy34FgDVdodn8YOmPlj4fLWuez1M3uo6nrvpsfvT0+eZ67nnrNNdtfsjd5sYdG1nxjxWFYyrS33xlhcqjMWT2O59i9XP9CtmrO15l7YtrW+LJEXOuZZlKeU0r1maeNta/u543Vue8cKxVz92qZQXaLLd1u9axZd2W5POW8F7LVV5yvQJ9hsLbZsFlBX4Zb03DGna9sisrpub5ErerrNiN/Mtyxey5y6Oxp5XnqVMOFtdPCe4/bH8/6pYwvxmba+ciIlIOKy5csdTdJ7RmndhG9B/b72Msv2B59oi5yAg617J8bUTrt3aEYljJRxi5RmP5ruUtub+RPpfS30KjllwxZi0vMHrJtXzhwoVMmjQpb8wts7lHLNF+FFuWqdDIp9TnK1S++OnFnHrqqUWfu1g7pa7XVm0dpP3tb3/j5FNOLnhEGy3PewRc6vaZ4/0X1dYjnXzrPffcc3zyk59MjzWl4DZX6nac0UbJRwV5jgjyHWXmq/OxCz9Ga8WW6JtkHQJ23BFsLELpb7VV060q9s2nInpW96RPjz4d8lyZybojTuH07tabAQcMyLvc3Ul40ylMSLjjTvKP5HQitcwT2WWJZMWW6Yj0pBYtJ+dMy2Aqd930xJi0v/ejV/WhufuWt9cFBgIF18msm+/UTe510ncgLRXT67d/kBDbqZvDh33cfzrnydRGlNywmjao6GNTOUAiEa3XslGlrZtI3ziTy1MbKtF/csvGm5qLTLds1E0FTas1PW96XW+eJrXe229vYcCA/ukvaqEXmPwbA0XrelbN9OfKrldyTLmOpBx27NjBwQcfnCPCfAW5R7XZb5Qc62WN2ktcv8T1MuPatauBAw/smXMbatreHCeRaEmKzeXR7TKRvR03bS/RbbYYyxgT5EpuLXWyM2JmnURjAqqqmrfVprgT8aQCaaWNPz6j85y62bbbueHh5e1qo8qgyowqMyw1nflYZck3RlXz9m9pbxyz9FFD5pupuW6Oei2jc9LeVGbG++8n2PlWQ1obqWdvmc4xeMv1Js5qI8ebOueIp0BCSC8r/Jz5Dj/T4s5oG8tYr6mlnH3OqlW0TvqyzNNQhdrOvV60vHrPe/Q/eP+W7auqaRtKbU80bWOWVadpWct2l6pTZc3bRst2m1on9eS5ds5Nhbl2UtEdb7E60YUbX9vEEUcMwWiKgeZpmvvY9B5qiTGtLDXd0ufI+yzyf4i+p0oe1eb5HxRsIzLz8svrGTEi/+d/BU8HFdrOSlwp7/u2hKOZfPWjk2f/OH+M+cSW6If0quLpGyZHEnJk46hqeSNUpXoenW/awD7KkufoJ8UdRsUk+3dy3GFURLJvrRowdSp1dW9RW/uJuMOomLq9G6k9dVjcYVTM2W1YJ7ZEX20w8KAecT29iEiXoS9MiYgEToleRCRwSvQiIoFTohcRCZwSvYhI4JToRUQCp0QvIhI4JXoRkcAp0YuIBE6JXkQkcEr0IiKBU6IXEQmcEr2ISOCU6EVEAqdELyISOCV6EZHAKdGLiAROiV5EJHBK9CIigVOiFxEJnBK9iEjglOhFRAKnRC8iEjglehGRwCnRi4gEToleRCRwSvQiIoFTohcRCVxJid7MpprZWjNbb2bX5Vj+MTNbYGb/MLOXzOwL5Q9VRETaomiiN7NqYCZwOnAMcI6ZHZNR7XvAPHcfD8wAflXuQEVEpG1KGdGfCKx39w3uvgeYA0zPqOPAQanp3sAb5QtRRETao1sJdQYBmyLz9cAnM+rcAvzJzK4EDgQ+V5boRESk3UpJ9JajzDPmzwHudff/MLOTgd+a2Sh3T6Q1ZHYpcClA//79qaura0PInUNDQ4P610mF3DdQ/7qiUhJ9PTAkMj+Y7FMzFwNTAdz9GTPrAfQD3o5WcvdZwCyAkSNHem1tbdui7gTq6upQ/zqnkPsG6l9XVMo5+iXACDMbZmb7kfywdX5GndeAyQBm9gmgB7ClnIGKiEjbFE307r4PuAJ4AlhN8uqalWZ2q5lNS1X7F+ASM3sR+ANwobtnnt4REZEYlHLqBnd/FHg0o+zmyPQq4NTyhiYiIuWgb8aKiAROiV5EJHBK9CIigVOiFxEJnBK9iEjglOhFRAKnRC8iEjglehGRwCnRi4gEToleRCRwSvQiIoFTohcRCZwSvYhI4JToRUQCp0QvIhI4JXoRkcAp0YuIBE6JXkQkcEr0IiKBU6IXEQmcEr2ISOCU6EVEAqdELyISuG5xByAiXcvevXupr69n9+7dFWm/d+/erF69uiJtd6QePXowePBgampq2t2WEr2IdKj6+np69erF0KFDMbOyt79r1y569epV9nY7kruzbds26uvrGTZsWLvb06kbEelQu3fv5pBDDqlIkg+FmXHIIYeU7ahHiV5EOpySfHHl/B8p0YuIBE6JXkQkcEr0ItLlfOlLX+L444/n2GOPZdasWQA8/vjjHHfccYwdO5bJkycD0NDQwEUXXcTo0aMZM2YMDz74YJxht5muuhGR2Pzb/1vJqjfeLWubI/rtzw++Mq5gnXvuuYe+ffvywQcfcMIJJzB9+nQuueQSFi1axLBhw3jnnXcAuO222+jduzfLly8HYPv27WWNtaMo0YtIl/OLX/yChx9+GIBNmzYxa9YsJk6c2HwpY9++fQF48sknmTNnTvN6ffr06fhgy0CJXkRi8/0vHlv2Nnft2lVweV1dHU8++STPPPMMBxxwALW1tYwdO5a1a9dm1XX3IK4Q0jl6EelSdu7cSZ8+fTjggANYs2YNzz77LB9++CELFy7k1VdfBWg+dXPaaafxy1/+snndznrqRoleRLqUqVOnsm/fPsaMGcNNN93ESSedRP/+/Zk1axZnnnkmY8eO5eyzzwbge9/7Htu3b2fUqFGMHTuWBQsWxBx92+jUjYh0Kd27d+exxx7Luez0009Pm+/Zsyf33XdfR4RVURrRi4gEToleRCRwSvQiIoErKdGb2VQzW2tm683sujx1vmpmq8xspZn9vrxhiohIWxX9MNbMqoGZwOeBemCJmc1391WROiOA64FT3X27mQ2oVMAiItI6pYzoTwTWu/sGd98DzAGmZ9S5BJjp7tsB3P3t8oYpIiJtVcrllYOATZH5euCTGXU+DmBmTwPVwC3u/nhmQ2Z2KXApQP/+/amrq2tDyJ1DQ0OD+tdJhdw3iL9/vXv3Lvrt1fZobGws2v5hhx3G5s2bKxZDuezevbssr1UpiT7X9389RzsjgFpgMPBXMxvl7jvSVnKfBcwCGDlypNfW1rY23k6jrq4O9a9zCrlvEH//Vq9eXdGf+iv1pwQ7w88N9ujRg/Hjx7e7nVJO3dQDQyLzg4E3ctT5o7vvdfdXgbUkE7+IyEeWu3PNNdcwatQoRo8ezdy5cwHYvHkzEydOZNy4cYwaNYq//vWvNDY2cuGFFzbXvfPOO2OOvnSljOiXACPMbBjwOjAD+FpGnf8LnAPca2b9SJ7K2VDOQEUkQI9dB28uL2uT3Q8ZCdN+VlLdhx56iGXLlvHiiy+ydetWTjjhBCZOnMjvf/97pkyZwo033khjYyPvv/8+y5Yt4/XXX2fFihUA7Nixo0jrHx1FR/Tuvg+4AngCWA3Mc/eVZnarmU1LVXsC2GZmq4AFwDXuvq1SQYuIlMPixYs555xzqK6uZuDAgUyaNIklS5Zwwgkn8Jvf/IZbbrmF5cuX06tXL4YPH86GDRu48sorefzxxznooIPiDr9kJd3rxt0fBR7NKLs5Mu3Ad1J/IiKlOf3fy97kh7t2sV+JdZOpK9vEiRNZtGgRjzzyCOeffz7XXHMNX//613nxxRd54oknmDlzJvPmzeOee+4pX+AVpG/GikiXNXHiRObOnUtjYyNbtmxh0aJFnHjiiWzcuJEBAwZwySWXcPHFF/PCCy+wdetWEokEX/nKV7jtttt44YUX4g6/ZLp7pYh0WV/+8pd55plnGDt2LGbGT37yEw499FDuu+8+7rjjDmpqaujZsyf3338/r7/+OhdddBGJRAKAH/3oRzFHXzolehHpchoaGgAwM+644w7uuOOOtOUXXHABF1xwQdZ6nWkUH6VTNyIigVOiFxEJnBK9iEjglOhFRAKnRC8iEjglehGRwCnRi4gEToleRKSAnj17xh1CuynRi4gETt+MFZHY/PjvP2bNO2vK2ubwnsO56VM35V1+7bXXcsQRR3D55ZcDcMstt2BmLFq0iO3bt7N3715+8IMfMH165i+mZmtoaGD69Ok517v//vv56U9/ipkxZswYfvvb3/LWW29x2WWXsWFD8i7ud911F6ecckoZel2YEr2IdCkzZszg29/+dnOinzdvHo8//jhXX301Bx10EFu3buWkk05i2rRpmOX6gb0WPXr04OGHH85ab9WqVdx+++08/fTT9OvXj3feeQeAq666ikmTJvHwww/T2NjYfCuGSlOiF5HYXHvitWVvs9jvxY4fP563336bN954gy1bttCnTx8OO+wwrr76ahYtWkRVVRWvv/46b731FoceemjBttydG264IWu9p556irPOOot+/foB0LdvXwCeeuop7r//fgCqq6vp3bt3GXpcnBK9iHQ5Z511Fg888ABvvvkmM2bMYPbs2WzZsoWlS5dSU1PD0KFD2b17d9F28q3n7kWPBjqSPowVkS5nxowZzJkzhwceeICzzjqLnTt3MmDAAGpqaliwYAEbN24sqZ18602ePJl58+axbVvyh/aaTt1MnjyZu+66C4DGxkbefffdCvQumxK9iHQ5xx57LLt27WLQoEEcdthhnHvuuTz//PNMmDCB2bNnc/TRR5fUTr71jj32WG688UYmTZrE2LFj+c53kj++9/Of/5wFCxYwevRojj/+eFauXFmxPkbp1I2IdEnLl7f8KHm/fv145plnctYr9IFpofVy3dN+4MCB/PGPf2xDtO2jEb2ISOA0ohcRKWL58uWcf/75aWXdu3fnueeeiymi1lGiFxEpYvTo0SxbtizuMNpMp25ERAKnRC8iEjglehGRwCnRi4gEToleRKSAQvej/+c//8moUaM6MJq2UaIXEQmcLq8Ukdi8+cMf8uHq8t6PvurII+l1y/fzLi/n/eijdu/ezTe+8Q2ef/55unXrxs9+9jM+85nPsHLlSi666CL27NlDIpHgwQcf5PDDD+erX/0q9fX1NDY2ctNNN3H22We3q9+FKNGLSJdSzvvRR82cORNIfrlqzZo1nHbaaaxbt467776bb33rW5x77rns2bOHxsZGHn30UQ4//HAeeeQRIHlztEpSoheR2Bx6ww1lb7Mj70cftXjxYq688koAjj76aI444gjWrVvHySefzO233059fT1nnnkmI0aMYPTo0Xz3u9/l2muv5YwzzuDTn/50u/pcjM7Ri0iX03Q/+rlz52bdj37ZsmUMHDiwpPvRR7l7zvKvfe1rzJ8/n/33358pU6bw1FNP8fGPf5ylS5cyevRorr/+em699dZydCsvjehFpMuZMWMGl1xyCVu3bmXhwoXMmzevTfejj5o4cSKzZ8/ms5/9LOvWreO1115j5MiRbNiwgeHDh3PVVVexYcMGXnrpJY4++mj69u3LeeedR8+ePbn33nvL38kIJXoR6XJy3Y/+i1/8IhMmTGDcuHEl348+6vLLL+eyyy5j9OjRdOvWjXvvvZfu3bszd+5cfve731FTU8Ohhx7KzTffzJIlS7jmmmuoqqqipqam+cdIKkWJXkS6pHLcj37o0KGsWLECSP5QeK6R+fXXX8/111+fVjZlyhSmTJnShqjbRufoRUQCpxG9iEgRuh+9iEgruXurrlGPWxz3o893FU9blHTqxsymmtlaM1tvZtcVqHeWmbmZTShbhCISlB49erBt27ayJrLQuDvbtm2jR48eZWmv6IjezKqBmcDngXpgiZnNd/dVGfV6AVcBneNYRkRiMXjwYOrr69myZUtF2t+9e3fZEmScevToweDBg8vSVimnbk4E1rv7BgAzmwNMB1Zl1LsN+Anw3bJEJiJBqqmpYdiwYRVrv66ujvHjx1es/c6olFM3g4BNkfn6VFkzMxsPDHH3/y5jbCIiUgaljOhzfWLSfHLNzKqAO4ELizZkdilwKUD//v2pq6srKcjOqKGhQf3rpELuG6h/XVEpib4eGBKZHwy8EZnvBYwC6lKfoh8KzDezae7+fLQhd58FzAIYOXKk19bWtj3yj7i6ujrUv84p5L6B+tcVlXLqZgkwwsyGmdl+wAxgftNCd9/p7v3cfai7DwWeBbKSvIiIxKNoonf3fcAVwBPAamCeu680s1vNbFqlAxQRkfYp6QtT7v4o8GhG2c156ta2PywRESkX3etGRCRwSvQiIoFTohcRCZwSvYhI4JToRUQCp0QvIhI4JXoRkcAp0YuIBE6JXkQkcEr0IiKBU6IXEQmcEr2ISOCU6EVEAqdELyISOCV6EZHAKdGLiAROiV5EJHBK9CIigVOiFxEJnBK9iEjglOhFRAKnRC8iEjglehGRwCnRi4gEToleRCRwSvQiIoFTohcRCZwSvYhI4JToRUQCp0QvIhI4JXoRkcAp0YuIBE6JXkQkcEr0IiKBU6IXEQmcEr2ISOCU6EVEAqdELyISuJISvZlNNbO1ZrbezK7Lsfw7ZrbKzF4ys7+Y2RHlD1VERNqiaKI3s2pgJnA6cAxwjpkdk1HtH8AEdx8DPAD8pNyBiohI25Qyoj8RWO/uG9x9DzAHmB6t4O4L3P391OyzwODyhikiIm1VSqIfBGyKzNenyvK5GHisPUGJiEj5dCuhjuUo85wVzc4DJgCT8iy/FLgUoH///tTV1ZUWZSfU0NCg/nVSIfcN1L+uqJREXw8MicwPBt7IrGRmnwNuBCa5+4e5GnL3WcAsgJEjR3ptbW1r4+006urqUP86p5D7BupfV1TKqZslwAgzG2Zm+wEzgPnRCmY2Hvg1MM3d3y5/mCIi0lZFE7277wOuAJ4AVgPz3H2lmd1qZtNS1e4AegL/ZWbLzGx+nuZERKSDlXLqBnd/FHg0o+zmyPTnyhyXiIiUib4ZKyISuJJG9JWw/webYc65UFUNVTVQ1Q2quyUfm+arqqG6JlIWnY/85S2Ltt00363AX3V2e5broiMRkc4jtkRf5ftg+z+hcS8k9kFiLyQaI/ONqbJ9ybLcV3RWnlVl73yadya5dhTJeuN2NcCrfaGqCqw6ucyqU+1FHyPLMuvmK8/bRhVgyUdreqzKM99UN0d5vnVpKe+9YwVs3C+9jeZpS4+jlOnm+cy2ck03rU8J7Wb2Uztu6XpiS/TvHTAEvvF06SskEi2JP7EPGvdFdhA55tN2GqmdSNO6afUL/UXb2BdpY2/GfHZMbh8ADvv2gDcm63ojeCLZl2hZoqm8MaM8T724dnoR4wGWxR1FOxTYmXwqkYBneySP7Kr3aznKa57er+Wor2m6uia5k0+bLrasW0u5O82va9N03jLSl3tkeyhUL7V88KZX4Nk1kUFLNVlHtZajLFd9K9BGtK2sgYN2uB0ptkTfalVVUNUd6B53JCV5sZLX8rpn7xQ8kfpLvbGb5yN/eEa9XNOZdTOXOXgjy5YtY9zYMS1lTQmleTqyLp5nmozyRPb6WdP52io2Xazdlng3b3qNIYcfBo17kjv1xtRfdLpxT3Knvvf9lsFA457cdROp+h8RRwG8EncUuY4kixyBlrh8wnvvweqeLU8V3REmCzJmW7M81yArEktVZjyRnVxVdSTuzPKmPuUqzzwSb73Ok+ilhVlyNBjjy7djYwKG18b2/JX0Sl0dQ8q9k27aOTcl/cZ96TuS6KkriEznKYOM5bnKyFn218WL+fQpJ6cfkTYNGjKPaDPLctZLZB8Ne0ZZoQFI1kCiPcsT7G7cQs8+/Qu/HllHFFb68sxl+WJKNOYoyyxvbOlXzvKmMwCR8jZQohfpCE075+puULN/rKE0djsQDugbawyVtCL0b8b+a+tPe+nyShGRwCnRi4gEToleRCRwXf4cvXv0So6WP08uzC5PLshRHr0MDmzXLvZt356cbrqcLPqHpR6ylxl5yttxSVpzP3P1C9L7m1Gn5aKDluX2wQc07tqV/kFh05RlTBR5TOtVvrq5O1VaWZ7yvBep7t1LYs+eirwOHaHoNr13L4kPP0zf1poePyL9LNiHRCLH9tkyb++9R+OOHS3tpDec97HlPdxcueg6zXK8v5Off+d674NVVWXVLykfVLVtbG5Z/4gOcmyvXj7/U59OT5RNf4kEjif/z6n55IvskMiRYCPrpW3QkQ0i54bTWeXZeHL2rzP386Ou0Jsyx/K8Ow7IP7BoZXmH9DUyn9anjHpp/4s878GsfiQSletDII5Zu2apu09ozTqxjeh9v/3ofvTI1B6vZe9mVZFLwcygKjK6aK5HqqwqrZ5FLyOrqsrYo2aum/E80fIcy1pb/vK6lxlx1FG0XKfdtKETeVOWUp69LGd52hst2key+0mOvjava5HZjL5F2n9l/SsceeSRqRczkmCap9N3NFmjpZw7onzrRPqXIeeoM+9ItLS6G17dwPChw2jT6xDpQ9brmrZeRrmlv0bZ22fh7a012/SGV19l+LBheeKNxNb8WkSWQc5tNVk1V39TK0Tfq/n6kfkeztWHyPs43//ilVde4agRI1peb8t8TH/tLW15a+o2VciznRTbfhKJtuWDK67I2maLiS3RN/brx+A774zr6Svug7o6+gZ8idfyujoOCbR/K+rq6Bdo3yD8/i0P/L3XlkSvD2NFRAKnRC8iEjglehGRwCnRi4gEToleRCRwSvQiIoFTohcRCZwSvYhI4GK7BYKZ7QLWxvLkHaMfsDXuICoo5P6F3DdQ/zq7ke7eqzUrxHlTs7WtvV9DZ2Jmz6t/nVPIfQP1r7Mzs+dbu45O3YiIBE6JXkQkcHEm+lkxPndHUP86r5D7BupfZ9fq/sX2YayIiHQMnboREQlcLInezKaa2VozW29m18URQyWY2RAzW2Bmq81spZl9K+6YKsHMqs3sH2b233HHUm5mdrCZPWBma1Kv48lxx1ROZnZ1attcYWZ/MLMeccfUHmZ2j5m9bWYrImV9zezPZvZy6rFPnDG2R57+3ZHaPl8ys4fN7OBi7XR4ojezamAmcDpwDHCOmR3T0XFUyD7gX9z9E8BJwDcD6lvUt4DVcQdRIT8HHnf3o4GxBNRPMxsEXAVMcPdRQDUwI96o2u1eYGpG2XXAX9x9BPCX1HxndS/Z/fszMMrdxwDrgOuLNRLHiP5EYL27b3D3PcAcYHoMcZSdu2929xdS07tIJolB8UZVXmY2GPgfwH/GHUu5mdlBwETg/wC4+x533xFvVGXXDdjfzLoBBwBvxBxPu7j7IuCdjOLpwH2p6fuAL3VoUGWUq3/u/id335eafRYYXKydOBL9IGBTZL6ewJIhgJkNBcYDz8UbSdn9b+BfgRB/xXk4sAX4TerU1H+a2YFxB1Uu7v468FPgNWAzsNPd/xRvVBUx0N03Q3LwBQyIOZ5K+l/AY8UqxZHoc/1yc1CX/phZT+BB4Nvu/m7c8ZSLmZ0BvO3uS+OOpUK6AccBd7n7eOA9Ovdhf5rUuerpwDDgcOBAMzsv3qikrczsRpKni2cXqxtHoq8HhkTmB9PJDx+jzKyGZJKf7e4PxR1PmZ0KTDOzf5I85fZZM/tdvCGVVT1Q7+5NR2EPkEz8ofgc8Kq7b3H3vcBDwCkxx1QJb5nZYQCpx7djjqfszOwC4AzgXC/hGvk4Ev0SYISZDTOz/Uh+GDQ/hjjKzsyM5Pnd1e7+s7jjKTd3v97dB7v7UJKv21PuHsyI0N3fBDaZ2chU0WRgVYwhldtrwElmdkBqW51MQB82R8wHLkhNXwD8McZYys7MpgLXAtPc/f1S1unwRJ/6EOEK4AmSG9k8d1/Z0XFUyKnA+SRHustSf1+IOyhplSuB2Wb2EjAO+GHM8ZRN6kjlAeAFYDnJ93+n/hapmf0BeAYYaWb1ZnYx8O/A583sZeDzqflOKU//fgn0Av6cyjF3F21H34wVEQmbvhkrIhI4JXoRkcAp0YuIBE6JXkQkcEr0IiKBU6IXaQMzqw3x7p0SJiV6EZHAKdFL0MzsPDP7e+qLJb9O3Uu/wcz+w8xeMLO/mFn/VN1xZvZs5D7ffVLlR5nZk2b2YmqdI1PN94zcu3526tumIh85SvQSLDP7BHA2cKq7jwMagXOBA4EX3P04YCHw/dQq9wPXpu7zvTxSPhuY6e5jSd4bZnOqfDzwbZK/q7bKQ7oAAAEhSURBVDCc5DejRT5yusUdgEgFTQaOB5akBtv7k7zBVQKYm6rzO+AhM+sNHOzuC1Pl9wH/ZWa9gEHu/jCAu+8GSLX3d3evT80vA4YCiyvfLZHWUaKXkBlwn7un/QKPmd2UUa/QfUAKnY75MDLdiN5P8hGlUzcSsr8AZ5nZAGj+LdEjSG73Z6XqfA1Y7O47ge1m9ulU+fnAwtTvCdSb2ZdSbXQ3swM6tBci7aQRiATL3VeZ2feAP5lZFbAX+CbJHxQ51syWAjtJnseH5C1t704l8g3ARany84Ffm9mtqTb+Zwd2Q6TddPdK6XLMrMHde8Ydh0hH0akbEZHAaUQvIhI4jehFRAKnRC8iEjglehGRwCnRi4gEToleRCRwSvQiIoH7/9vc2Fw4DZB4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2.plot(x='epoch', y=['acc', 'loss', 'val_acc', 'val_loss'], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
