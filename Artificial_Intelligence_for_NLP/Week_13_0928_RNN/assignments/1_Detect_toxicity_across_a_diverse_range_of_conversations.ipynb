{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jigsaw Unintended Bias in Toxicity Classification](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 'D:\\Github\\NLP\\Artificial_Intelligence_for_NLP\\Week_13_0928_RNN\\assignments\\temp' 将被用来存储语料和临时性字典\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "TEMP_FOLDER = os.path.abspath('./temp/')\n",
    "print(f\"文件夹 '{TEMP_FOLDER}' 将被用来存储语料和临时性字典\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s: %(levelname)s: %(message)s\", level=logging.INFO)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as  plt\n",
    "import matplotlib.patches as patches\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional, LSTM\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding files\n",
    "EMBEDDING_FILES = ['data/crawl-300d-2M.gensim', 'data/glove.840B.300d.gensim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "EMBEDDING_DIM = 300\n",
    "MAX_LEN = 220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTITY_COLUMNS = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', \n",
    "                  'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n",
    "AUX_COLUMS = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "STOP_CHARS = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chunk = df.get_chunk(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>254559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Is that a “Taurus revolver known as “The Judge...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>51627</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>254560</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Motives matter. Is Mueller posting as private ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51876</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>254561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[... continued]\\nMueller is at his best with \"...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>51876</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  target                                       comment_text  \\\n",
       "9997  254559     0.0  Is that a “Taurus revolver known as “The Judge...   \n",
       "9998  254560     0.0  Motives matter. Is Mueller posting as private ...   \n",
       "9999  254561     0.0  [... continued]\\nMueller is at his best with \"...   \n",
       "\n",
       "      severe_toxicity  obscene  identity_attack  insult  threat  asian  \\\n",
       "9997              0.0      0.0              0.0     0.0     0.0    0.0   \n",
       "9998              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "9999              0.0      0.0              0.0     0.0     0.0    NaN   \n",
       "\n",
       "      atheist            ...             article_id    rating  funny  wow  \\\n",
       "9997      0.0            ...                  51627  approved      0    0   \n",
       "9998      NaN            ...                  51876  approved      0    0   \n",
       "9999      NaN            ...                  51876  approved      0    0   \n",
       "\n",
       "      sad  likes  disagree  sexual_explicit  identity_annotator_count  \\\n",
       "9997    0      0         0              0.0                         4   \n",
       "9998    0      2         0              0.0                         0   \n",
       "9999    0      2         0              0.0                         0   \n",
       "\n",
       "      toxicity_annotator_count  \n",
       "9997                         4  \n",
       "9998                         4  \n",
       "9999                         4  \n",
       "\n",
       "[3 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunk.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'target', 'comment_text', 'severe_toxicity', 'obscene',\n",
       "       'identity_attack', 'insult', 'threat', 'asian', 'atheist', 'bisexual',\n",
       "       'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu',\n",
       "       'homosexual_gay_or_lesbian', 'intellectual_or_learning_disability',\n",
       "       'jewish', 'latino', 'male', 'muslim', 'other_disability',\n",
       "       'other_gender', 'other_race_or_ethnicity', 'other_religion',\n",
       "       'other_sexual_orientation', 'physical_disability',\n",
       "       'psychiatric_or_mental_illness', 'transgender', 'white', 'created_date',\n",
       "       'publication_id', 'parent_id', 'article_id', 'rating', 'funny', 'wow',\n",
       "       'sad', 'likes', 'disagree', 'sexual_explicit',\n",
       "       'identity_annotator_count', 'toxicity_annotator_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunk.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data params\n",
    "# train_df = pd.csv('data/train.csv') # to much data, just run a demo\n",
    "train_df = df_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97315</th>\n",
       "      <td>7194635</td>\n",
       "      <td>He should lose his job for promoting mis-infor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97316</th>\n",
       "      <td>7194636</td>\n",
       "      <td>\"Thinning project is meant to lower fire dange...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97317</th>\n",
       "      <td>7194637</td>\n",
       "      <td>I hope you millennials are happy that you put ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97318</th>\n",
       "      <td>7194638</td>\n",
       "      <td>I'm thinking Kellyanne Conway (a.k.a. The Trum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97319</th>\n",
       "      <td>7194639</td>\n",
       "      <td>I still can't figure why a pizza in AK cost mo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                       comment_text\n",
       "97315  7194635  He should lose his job for promoting mis-infor...\n",
       "97316  7194636  \"Thinning project is meant to lower fire dange...\n",
       "97317  7194637  I hope you millennials are happy that you put ...\n",
       "97318  7194638  I'm thinking Kellyanne Conway (a.k.a. The Trum...\n",
       "97319  7194639  I still can't figure why a pizza in AK cost mo..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df[TEXT_COLUMN].astype(str)\n",
    "y_train = train_df[TARGET_COLUMN].values\n",
    "y_aux_train = train_df[AUX_COLUMS].values\n",
    "x_test = test_df[TEXT_COLUMN].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n",
    "    train_df[column] = np.where(train_df[column] >= .5, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ Integrity means that you pay your debts.]\\n\\nDoes this apply to President Trump too?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(filters=STOP_CHARS, lower=False)\n",
    "tokenizer.fit_on_texts(x_train.tolist() + x_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "padded_x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = np.ones(len(x_train), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1027 17:31:44.629886  5696 utils.py:141] NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "sample_weights /= sample_weights.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix(word_index, path):\n",
    "    embedding_file = KeyedVectors.load(path, mmap='r')\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "    for word, index in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[index] = embedding_file.wv[word.lower()]\n",
    "        except:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1027 16:52:46.610595  2524 utils.py:422] loading Word2VecKeyedVectors object from data/crawl-300d-2M.gensim\n",
      "I1027 16:52:56.458011  2524 utils.py:461] loading vectors from data/crawl-300d-2M.gensim.vectors.npy with mmap=r\n",
      "I1027 16:52:56.521013  2524 utils.py:494] setting ignored attribute vectors_norm to None\n",
      "I1027 16:52:56.522010  2524 utils.py:428] loaded data/crawl-300d-2M.gensim\n",
      "I1027 16:58:47.366861  2524 utils.py:422] loading Word2VecKeyedVectors object from data/glove.840B.300d.gensim\n",
      "I1027 16:58:58.194862  2524 utils.py:461] loading vectors from data/glove.840B.300d.gensim.vectors.npy with mmap=r\n",
      "I1027 16:58:58.252863  2524 utils.py:494] setting ignored attribute vectors_norm to None\n",
      "I1027 16:58:58.253862  2524 utils.py:428] loaded data/glove.840B.300d.gensim\n"
     ]
    }
   ],
   "source": [
    "# np.concatenate(axis=-1) align to last axis\n",
    "embedding_matrix = np.concatenate([build_matrix(word2index, f) for f in EMBEDDING_FILES],\n",
    "                                 axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1027 17:32:19.032387  5696 utils.py:422] loading Word2VecKeyedVectors object from data/crawl-300d-2M.gensim\n",
      "I1027 17:32:24.629389  5696 utils.py:461] loading vectors from data/crawl-300d-2M.gensim.vectors.npy with mmap=r\n",
      "I1027 17:32:24.634388  5696 utils.py:494] setting ignored attribute vectors_norm to None\n",
      "I1027 17:32:24.635388  5696 utils.py:428] loaded data/crawl-300d-2M.gensim\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = build_matrix(word2index, EMBEDDING_FILES[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101242, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "#     tf.reset_default_graph()\n",
    "    model = Sequential([\n",
    "        Embedding(*embedding_matrix.shape, input_length=MAX_LEN, \n",
    "                  weights=[embedding_matrix], trainable=False),\n",
    "        SpatialDropout1D(.2),\n",
    "        Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)),\n",
    "        Bidirectional(LSTM(LSTM_UNITS, return_sequences=True)),\n",
    "#         concatenate([GlobalMaxPooling1D(), GlobalAveragePooling1D()]),\n",
    "        Dense(DENSE_HIDDEN_UNITS, activation='relu'),\n",
    "        Dense(DENSE_HIDDEN_UNITS, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 220, 300)          30372600  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_12 (Spatia (None, 220, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 220, 256)          439296    \n",
      "_________________________________________________________________\n",
      "bidirectional_24 (Bidirectio (None, 220, 256)          394240    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 220, 512)          131584    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 220, 512)          262656    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 220, 1)            513       \n",
      "=================================================================\n",
      "Total params: 31,600,889\n",
      "Trainable params: 1,228,289\n",
      "Non-trainable params: 30,372,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(embedding_matrix, 5)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_new(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x)\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1027 17:50:05.526159  5696 deprecation_wrapper.py:119] From C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W1027 17:50:05.556161  5696 deprecation.py:323] From C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 797s - loss: 0.7635 - dense_12_loss: 0.5660 - dense_13_loss: 0.1975\n",
      "Epoch 1/1\n",
      " - 944s - loss: 0.5961 - dense_12_loss: 0.5020 - dense_13_loss: 0.0940\n",
      "Epoch 1/1\n",
      " - 1461s - loss: 0.5440 - dense_12_loss: 0.4569 - dense_13_loss: 0.0871\n",
      "Epoch 1/1\n",
      " - 1664s - loss: 0.5102 - dense_12_loss: 0.4294 - dense_13_loss: 0.0808\n",
      "Epoch 1/1\n",
      " - 1912s - loss: 0.7564 - dense_16_loss: 0.5606 - dense_17_loss: 0.1958\n",
      "Epoch 1/1\n",
      " - 2124s - loss: 0.5939 - dense_16_loss: 0.4981 - dense_17_loss: 0.0958\n",
      "Epoch 1/1\n",
      " - 2361s - loss: 0.5417 - dense_16_loss: 0.4552 - dense_17_loss: 0.0864\n",
      "Epoch 1/1\n",
      " - 2554s - loss: 0.5139 - dense_16_loss: 0.4323 - dense_17_loss: 0.0815\n"
     ]
    }
   ],
   "source": [
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    \n",
    "    model = build_model_new(embedding_matrix, y_aux_train.shape[-1])\n",
    "    \n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(padded_x_train, \n",
    "                  [y_train, y_aux_train], \n",
    "                  batch_size=BATCH_SIZE, \n",
    "                  epochs=1,\n",
    "                  verbose=2, \n",
    "                  sample_weight=[sample_weights.values, np.ones_like(sample_weights)])\n",
    "        \n",
    "        checkpoint_predictions.append(model.predict(padded_x_test, \n",
    "                                                    batch_size=2048)[0].flatten())\n",
    "        \n",
    "        weights.append(2 ** global_epoch)\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75, 2.75, 4.75, 6.75])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.arange(8).reshape((4,-1))\n",
    "np.average(data, axis=1, weights=[1./4, 3./4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 2.5, 4.5, 6.5])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(data, axis=1, weights=[2./4, 2./4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 2.5, 4.5, 6.5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8405555555555555"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(797+944+1461+1664+1921+2124+2361+2554)/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.27149367, 0.25406474, 0.2540115 , ..., 0.27283764, 0.2295815 ,\n",
       "        0.16598171], dtype=float32),\n",
       " array([0.11059389, 0.10830662, 0.30880198, ..., 0.20929837, 0.10653675,\n",
       "        0.0228349 ], dtype=float32),\n",
       " array([0.10825571, 0.21817312, 0.47729072, ..., 0.47704375, 0.1608783 ,\n",
       "        0.0623906 ], dtype=float32),\n",
       " array([0.04029423, 0.10366952, 0.27328348, ..., 0.40838522, 0.06009936,\n",
       "        0.02198204], dtype=float32),\n",
       " array([0.31031466, 0.30524558, 0.32145077, ..., 0.33725137, 0.28382364,\n",
       "        0.20742115], dtype=float32),\n",
       " array([0.24327421, 0.25667733, 0.5568447 , ..., 0.44904125, 0.29117572,\n",
       "        0.10760072], dtype=float32),\n",
       " array([0.16006488, 0.32627714, 0.5602039 , ..., 0.61189336, 0.25650308,\n",
       "        0.18900347], dtype=float32),\n",
       " array([0.05497602, 0.12771028, 0.38922197, ..., 0.5216385 , 0.10533962,\n",
       "        0.03966653], dtype=float32)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 8, 1, 2, 4, 8]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just run a demo with 10000 train_data spend nearly 4 hours..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df.id,\n",
    "    'prediction': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
