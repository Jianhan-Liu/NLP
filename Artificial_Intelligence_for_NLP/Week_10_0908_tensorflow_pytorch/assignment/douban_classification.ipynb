{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "from datetime import datetime\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, scale, normalize, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/movie_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>261497</td>\n",
       "      <td>261497</td>\n",
       "      <td>261497</td>\n",
       "      <td>261495</td>\n",
       "      <td>261497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>260150</td>\n",
       "      <td>2761</td>\n",
       "      <td>2760</td>\n",
       "      <td>213970</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>9</td>\n",
       "      <td>https://movie.douban.com/subject/1849031/</td>\n",
       "      <td>当幸福来敲门 The Pursuit of Happyness</td>\n",
       "      <td>经典</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>6</td>\n",
       "      <td>396</td>\n",
       "      <td>396</td>\n",
       "      <td>200</td>\n",
       "      <td>43002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                       link  \\\n",
       "count   261497                                     261497   \n",
       "unique  260150                                       2761   \n",
       "top          9  https://movie.douban.com/subject/1849031/   \n",
       "freq         6                                        396   \n",
       "\n",
       "                                   name comment    star  \n",
       "count                            261497  261495  261497  \n",
       "unique                             2760  213970      11  \n",
       "top     当幸福来敲门 The Pursuit of Happyness      经典       4  \n",
       "freq                                396     200   43002  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_logloss(actual, predicted, eps=1e-15):\n",
    "    \"\"\"对数损失度量（Logarithmic Loss  Metric）的多分类版本。\n",
    "    :param actual: 包含actual target classes的数组\n",
    "    :param predicted: 分类预测结果矩阵, 每个类别都有一个概率\n",
    "    \"\"\"\n",
    "    # Convert 'actual' to a binary array if it's not already:\n",
    "    if len(actual.shape) == 1:\n",
    "        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n",
    "        for i, val in enumerate(actual):\n",
    "            actual2[i, val] = 1\n",
    "        actual = actual2\n",
    "\n",
    "    clip = np.clip(predicted, eps, 1 - eps)\n",
    "    rows = actual.shape[0]\n",
    "    vsota = np.sum(actual * np.log(clip))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评论的句子有点儿短，去掉一些停用词后可能丢失信息，试试不用停用词，简单去掉标点后分词\n",
    "with open('data/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    STOPWORDS = set([line[:-1] for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence_drop_stopwords(sentence):\n",
    "    return list(set(jieba.cut(sentence)) - STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    result = []\n",
    "    segment = re.findall('\\d*\\w+', sentence)\n",
    "    result += [jieba.lcut(x) for x in segment]\n",
    "    return [x for each in result for x in each if not x.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['感人至深', '的', '电影']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentence(df_new.comment[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['首映礼', '看', '的', '首映礼', '看', '的']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['首映礼', '看', '的']\n",
    "b = ['首映礼', '看', '的']\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_model\n",
    "wiki_model_path = r\"D:\\Github\\NLP\\Artificial_Intelligence_for_NLP\\Week_04_0727_word2vec\\Assignment\\word2vec_wiki.model\"\n",
    "wiki_model = Word2Vec.load(wiki_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用句子中词汇的平均词向量作为句子词向量\n",
    "def sentence_vector(sentence):\n",
    "    flag = 0\n",
    "    sentence_vec = np.zeros(wiki_model.vector_size)\n",
    "    nums = len(sentence)\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sentence_vec += wiki_model.wv[word]\n",
    "            flag = 1\n",
    "        except KeyError:\n",
    "            nums -= 1\n",
    "    return np.nan if not flag else sentence_vec / nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预处理 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### star 属性应该只有1-5数字或者字符串选其一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '4', '5', '3', 'star', 4, 3, 5, 2, 1], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.star.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除comment为nan行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 4, 5, 3], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = df.copy()\n",
    "df_new.drop(df_new[df_new.star=='star'].index, inplace=True)\n",
    "df_new.dropna(subset=['comment'],inplace=True)\n",
    "df_new.star = df_new.star.apply(lambda x: int(x))\n",
    "df_new.reset_index(drop=True, inplace=True)\n",
    "df_new.star.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['clean_comment'] = df_new.comment.apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['comment_vec'] = df_new.clean_comment.apply(sentence_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>comment_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>261489</th>\n",
       "      <td>260145</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>内容只能说一般。。女性角色怎么都这么悲催啊！！不过男猪脚很帅</td>\n",
       "      <td>3</td>\n",
       "      <td>[内容, 只能, 说, 一般, 女性, 角色, 怎么, 都, 这么, 悲, 催, 啊, 不过...</td>\n",
       "      <td>[-0.9653091602958739, -0.5943305254913867, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261490</th>\n",
       "      <td>260146</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>翘了三天班就窝在家里看小基片惹（手动拜拜.gif</td>\n",
       "      <td>3</td>\n",
       "      <td>[翘, 了, 三天, 班, 就, 窝, 在, 家里, 看小, 基片, 惹, 手动, 拜拜, ...</td>\n",
       "      <td>[-0.1373161240057512, -0.9519095800139687, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261491</th>\n",
       "      <td>260147</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>我喜欢女主角，希腊雕塑一般的面庞与身体。（在一部同志题材的电影中迷恋女主角好像很不应该吧）</td>\n",
       "      <td>2</td>\n",
       "      <td>[我, 喜欢, 女主角, 希腊, 雕塑, 一般, 的, 面庞, 与, 身体, 在, 一部, ...</td>\n",
       "      <td>[-0.49309063826998073, -0.4821888374475141, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261492</th>\n",
       "      <td>260148</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>冲着颜值还可以看下去</td>\n",
       "      <td>3</td>\n",
       "      <td>[冲着, 颜值, 还, 可以, 看, 下去]</td>\n",
       "      <td>[-0.23062036807338396, -0.8813576425115267, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261493</th>\n",
       "      <td>260149</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>除了主人公都不帅，女主挺漂亮之外……唯一的感觉就是他男朋友真让人恶心</td>\n",
       "      <td>3</td>\n",
       "      <td>[除了, 主人公, 都, 不帅, 女主挺, 漂亮, 之外, 唯一, 的, 感觉, 就是, 他...</td>\n",
       "      <td>[-1.4769697139660518, 0.1903096747895082, -0.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                       link  \\\n",
       "261489  260145  https://movie.douban.com/subject/1441763/   \n",
       "261490  260146  https://movie.douban.com/subject/1441763/   \n",
       "261491  260147  https://movie.douban.com/subject/1441763/   \n",
       "261492  260148  https://movie.douban.com/subject/1441763/   \n",
       "261493  260149  https://movie.douban.com/subject/1441763/   \n",
       "\n",
       "                             name  \\\n",
       "261489  不羁美少年 À cause d'un garçon   \n",
       "261490  不羁美少年 À cause d'un garçon   \n",
       "261491  不羁美少年 À cause d'un garçon   \n",
       "261492  不羁美少年 À cause d'un garçon   \n",
       "261493  不羁美少年 À cause d'un garçon   \n",
       "\n",
       "                                              comment  star  \\\n",
       "261489                 内容只能说一般。。女性角色怎么都这么悲催啊！！不过男猪脚很帅     3   \n",
       "261490                       翘了三天班就窝在家里看小基片惹（手动拜拜.gif     3   \n",
       "261491  我喜欢女主角，希腊雕塑一般的面庞与身体。（在一部同志题材的电影中迷恋女主角好像很不应该吧）     2   \n",
       "261492                                     冲着颜值还可以看下去     3   \n",
       "261493             除了主人公都不帅，女主挺漂亮之外……唯一的感觉就是他男朋友真让人恶心     3   \n",
       "\n",
       "                                            clean_comment  \\\n",
       "261489  [内容, 只能, 说, 一般, 女性, 角色, 怎么, 都, 这么, 悲, 催, 啊, 不过...   \n",
       "261490  [翘, 了, 三天, 班, 就, 窝, 在, 家里, 看小, 基片, 惹, 手动, 拜拜, ...   \n",
       "261491  [我, 喜欢, 女主角, 希腊, 雕塑, 一般, 的, 面庞, 与, 身体, 在, 一部, ...   \n",
       "261492                             [冲着, 颜值, 还, 可以, 看, 下去]   \n",
       "261493  [除了, 主人公, 都, 不帅, 女主挺, 漂亮, 之外, 唯一, 的, 感觉, 就是, 他...   \n",
       "\n",
       "                                              comment_vec  \n",
       "261489  [-0.9653091602958739, -0.5943305254913867, -0....  \n",
       "261490  [-0.1373161240057512, -0.9519095800139687, 1.0...  \n",
       "261491  [-0.49309063826998073, -0.4821888374475141, -0...  \n",
       "261492  [-0.23062036807338396, -0.8813576425115267, -0...  \n",
       "261493  [-1.4769697139660518, 0.1903096747895082, -0.6...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 删除转化后comment_vec为nan的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.dropna(subset=['comment_vec'], inplace=True)\n",
    "df_new.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>comment_vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250914</th>\n",
       "      <td>260145</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>内容只能说一般。。女性角色怎么都这么悲催啊！！不过男猪脚很帅</td>\n",
       "      <td>3</td>\n",
       "      <td>[内容, 只能, 说, 一般, 女性, 角色, 怎么, 都, 这么, 悲, 催, 啊, 不过...</td>\n",
       "      <td>[-0.9653091602958739, -0.5943305254913867, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250915</th>\n",
       "      <td>260146</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>翘了三天班就窝在家里看小基片惹（手动拜拜.gif</td>\n",
       "      <td>3</td>\n",
       "      <td>[翘, 了, 三天, 班, 就, 窝, 在, 家里, 看小, 基片, 惹, 手动, 拜拜, ...</td>\n",
       "      <td>[-0.1373161240057512, -0.9519095800139687, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250916</th>\n",
       "      <td>260147</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>我喜欢女主角，希腊雕塑一般的面庞与身体。（在一部同志题材的电影中迷恋女主角好像很不应该吧）</td>\n",
       "      <td>2</td>\n",
       "      <td>[我, 喜欢, 女主角, 希腊, 雕塑, 一般, 的, 面庞, 与, 身体, 在, 一部, ...</td>\n",
       "      <td>[-0.49309063826998073, -0.4821888374475141, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250917</th>\n",
       "      <td>260148</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>冲着颜值还可以看下去</td>\n",
       "      <td>3</td>\n",
       "      <td>[冲着, 颜值, 还, 可以, 看, 下去]</td>\n",
       "      <td>[-0.23062036807338396, -0.8813576425115267, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250918</th>\n",
       "      <td>260149</td>\n",
       "      <td>https://movie.douban.com/subject/1441763/</td>\n",
       "      <td>不羁美少年 À cause d'un garçon</td>\n",
       "      <td>除了主人公都不帅，女主挺漂亮之外……唯一的感觉就是他男朋友真让人恶心</td>\n",
       "      <td>3</td>\n",
       "      <td>[除了, 主人公, 都, 不帅, 女主挺, 漂亮, 之外, 唯一, 的, 感觉, 就是, 他...</td>\n",
       "      <td>[-1.4769697139660518, 0.1903096747895082, -0.6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                       link  \\\n",
       "250914  260145  https://movie.douban.com/subject/1441763/   \n",
       "250915  260146  https://movie.douban.com/subject/1441763/   \n",
       "250916  260147  https://movie.douban.com/subject/1441763/   \n",
       "250917  260148  https://movie.douban.com/subject/1441763/   \n",
       "250918  260149  https://movie.douban.com/subject/1441763/   \n",
       "\n",
       "                             name  \\\n",
       "250914  不羁美少年 À cause d'un garçon   \n",
       "250915  不羁美少年 À cause d'un garçon   \n",
       "250916  不羁美少年 À cause d'un garçon   \n",
       "250917  不羁美少年 À cause d'un garçon   \n",
       "250918  不羁美少年 À cause d'un garçon   \n",
       "\n",
       "                                              comment  star  \\\n",
       "250914                 内容只能说一般。。女性角色怎么都这么悲催啊！！不过男猪脚很帅     3   \n",
       "250915                       翘了三天班就窝在家里看小基片惹（手动拜拜.gif     3   \n",
       "250916  我喜欢女主角，希腊雕塑一般的面庞与身体。（在一部同志题材的电影中迷恋女主角好像很不应该吧）     2   \n",
       "250917                                     冲着颜值还可以看下去     3   \n",
       "250918             除了主人公都不帅，女主挺漂亮之外……唯一的感觉就是他男朋友真让人恶心     3   \n",
       "\n",
       "                                            clean_comment  \\\n",
       "250914  [内容, 只能, 说, 一般, 女性, 角色, 怎么, 都, 这么, 悲, 催, 啊, 不过...   \n",
       "250915  [翘, 了, 三天, 班, 就, 窝, 在, 家里, 看小, 基片, 惹, 手动, 拜拜, ...   \n",
       "250916  [我, 喜欢, 女主角, 希腊, 雕塑, 一般, 的, 面庞, 与, 身体, 在, 一部, ...   \n",
       "250917                             [冲着, 颜值, 还, 可以, 看, 下去]   \n",
       "250918  [除了, 主人公, 都, 不帅, 女主挺, 漂亮, 之外, 唯一, 的, 感觉, 就是, 他...   \n",
       "\n",
       "                                              comment_vec  \n",
       "250914  [-0.9653091602958739, -0.5943305254913867, -0....  \n",
       "250915  [-0.1373161240057512, -0.9519095800139687, 1.0...  \n",
       "250916  [-0.49309063826998073, -0.4821888374475141, -0...  \n",
       "250917  [-0.23062036807338396, -0.8813576425115267, -0...  \n",
       "250918  [-1.4769697139660518, 0.1903096747895082, -0.6...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all datasets:\t\t (250919, 200) (250919,)\n",
      "train_valid_datasets:\t (200735, 200) (200735,)\n",
      "test_datasets:\t\t (50184, 200) (50184,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_new.comment_vec.tolist())\n",
    "y = df_new.star\n",
    "train_valid_X, test_X, train_valid_y, test_y = train_test_split(X, y, test_size=.2)\n",
    "print('all datasets:\\t\\t', X.shape, y.shape)\n",
    "print('train_valid_datasets:\\t', train_valid_X.shape, train_valid_y.shape)\n",
    "print('test_datasets:\\t\\t', test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression()\n",
    "log_clf.fit(train_valid_X, train_valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.23      0.29     19027\n",
      "           2       0.32      0.02      0.04     21865\n",
      "           3       0.37      0.33      0.35     50631\n",
      "           4       0.37      0.70      0.49     64263\n",
      "           5       0.43      0.23      0.30     44949\n",
      "\n",
      "    accuracy                           0.38    200735\n",
      "   macro avg       0.38      0.30      0.29    200735\n",
      "weighted avg       0.38      0.38      0.34    200735\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred = log_clf.predict(train_valid_X)\n",
    "print(classification_report(train_valid_y, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(train_valid_y, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(conf_mat, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier()\n",
    "sgd_clf.fit(train_valid_X, train_valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.36      0.14      0.20     18575\n",
      "           2       0.14      0.06      0.08     21340\n",
      "           3       0.30      0.20      0.24     49131\n",
      "           4       0.35      0.75      0.47     63113\n",
      "           5       0.38      0.10      0.15     44113\n",
      "\n",
      "    accuracy                           0.33    196272\n",
      "   macro avg       0.30      0.25      0.23    196272\n",
      "weighted avg       0.32      0.33      0.27    196272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ypred_sgd = sgd_clf.predict(train_valid_X)\n",
    "print(classification_report(train_valid_y, ypred_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dir(prefix=''):\n",
    "    now = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    root_logdir = 'tf_logs'\n",
    "    if prefix:\n",
    "        prefix += '-'\n",
    "    name = prefix + 'run_' + now\n",
    "    return os.path.join(root_logdir, name)\n",
    "\n",
    "\n",
    "logdir = log_dir('douban_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_flatten, train_labels, test_flatten, test_labels):\n",
    "    reset_graph()\n",
    "    checkpoint_path = 'tmp/douban_comment'\n",
    "    units = 200\n",
    "    initial_learning_rate = .1\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                                 decay_steps=10000,\n",
    "                                                                 decay_rate=.96,\n",
    "                                                                 staircase=True)\n",
    "\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                     monitor='val_acc',\n",
    "                                                     verbose=1,\n",
    "                                                     save_best_only=True,\n",
    "                                                     load_weights_on_restart=True)\n",
    "    stop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "                                                     mode='max',\n",
    "                                                     patience=3)\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger('csv_logger')\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(logdir)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(4 * units, activation=tf.nn.relu, input_shape=(200, )),\n",
    "        tf.keras.layers.Dropout(.5),\n",
    "        tf.keras.layers.Dense(2 * units, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(.5),\n",
    "        tf.keras.layers.Dense(units, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dropout(.5),\n",
    "        tf.keras.layers.Dense(5, activation=tf.nn.softmax)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr_schedule),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(train_flatten, train_labels, validation_split=.2,\n",
    "              epochs=100, batch_size=1024, callbacks=[cp_callback,\n",
    "                                     stop_callback,\n",
    "                                     csv_logger,\n",
    "                                     tensorboard])\n",
    "\n",
    "    print(\"Model's performance on test dataset:\")\n",
    "    print(f\"Test accuracy: {model.evaluate(test_flatten, test_labels)[1] * 100:.3f}%\")\n",
    "    model.save('final_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((196272, 5), (49069, 5))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = OneHotEncoder()\n",
    "train_valid_y_1hot = encoder.fit_transform(train_valid_y.to_numpy().reshape(-1,1))\n",
    "test_y_1hot = encoder.fit_transform(test_y.to_numpy().reshape(-1,1))\n",
    "train_valid_y_1hot.shape, test_y_1hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 157017 samples, validate on 39255 samples\n",
      "Epoch 1/100\n",
      "156672/157017 [============================>.] - ETA: 0s - loss: 1.5631 - acc: 0.3093\n",
      "Epoch 00001: val_acc improved from -inf to 0.31988, saving model to tmp/douban_comment\n",
      "157017/157017 [==============================] - 11s 72us/sample - loss: 1.5629 - acc: 0.3093 - val_loss: 1.5183 - val_acc: 0.3199\n",
      "Epoch 2/100\n",
      "156672/157017 [============================>.] - ETA: 0s - loss: 1.5180 - acc: 0.3215\n",
      "Epoch 00002: val_acc improved from 0.31988 to 0.31996, saving model to tmp/douban_comment\n",
      "157017/157017 [==============================] - 11s 71us/sample - loss: 1.5180 - acc: 0.3215 - val_loss: 1.5156 - val_acc: 0.3200\n",
      "Epoch 3/100\n",
      "156672/157017 [============================>.] - ETA: 0s - loss: 1.5138 - acc: 0.3220- ETA: 1s - \n",
      "Epoch 00003: val_acc did not improve from 0.31996\n",
      "157017/157017 [==============================] - 10s 63us/sample - loss: 1.5139 - acc: 0.3221 - val_loss: 1.5148 - val_acc: 0.3198\n",
      "Epoch 4/100\n",
      "156672/157017 [============================>.] - ETA: 0s - loss: 1.5125 - acc: 0.3223\n",
      "Epoch 00004: val_acc did not improve from 0.31996\n",
      "157017/157017 [==============================] - 10s 63us/sample - loss: 1.5125 - acc: 0.3223 - val_loss: 1.5144 - val_acc: 0.3198\n",
      "Epoch 5/100\n",
      "156672/157017 [============================>.] - ETA: 0s - loss: 1.5118 - acc: 0.3228\n",
      "Epoch 00005: val_acc did not improve from 0.31996\n",
      "157017/157017 [==============================] - 10s 63us/sample - loss: 1.5119 - acc: 0.3227 - val_loss: 1.5143 - val_acc: 0.3198\n",
      "Model's performance on test dataset:\n",
      "49069/49069 [==============================] - 3s 56us/sample - loss: 1.5147 - acc: 0.3189\n",
      "Test accuracy: 31.886%\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "train(train_valid_X, train_valid_y_1hot, test_X, test_y_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
