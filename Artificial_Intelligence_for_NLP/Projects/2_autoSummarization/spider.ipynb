{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件夹 'D:\\Github\\NLP\\Artificial_Intelligence_for_NLP\\Projects\\2_autoSummarization\\data\\spider' 将被用来存储临时文件\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "TEMP_FOLDER = os.path.abspath('./data/spider/')\n",
    "print(f\"文件夹 '{TEMP_FOLDER}' 将被用来存储临时文件\")\n",
    "import logging\n",
    "logging.basicConfig(format=\"%(asctime)s: %(levelname)s: %(message)s\", level=logging.INFO)\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "seaborn.set()\n",
    "import matplotlib.pyplot as  plt\n",
    "import matplotlib.patches as patches\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from requests_futures.sessions import FuturesSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flatten(list_):\n",
    "    for s in list_:\n",
    "        if isinstance(s, list):\n",
    "            yield from _flatten(s)\n",
    "        else:\n",
    "            yield s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 0, 10, 11, 11, 12]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(_flatten([1,2,3,[4,[5,[6,[7,[8],0],10], 11],11],12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-24 22:49:10,819: WARNING: Error occurred during loading data. Trying to use cache server https://fake-useragent.herokuapp.com/browsers/0.1.11\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\urllib\\request.py\", line 1318, in do_open\n",
      "    encode_chunked=req.has_header('Transfer-encoding'))\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\http\\client.py\", line 1239, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\http\\client.py\", line 1285, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\http\\client.py\", line 1234, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\http\\client.py\", line 1026, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\http\\client.py\", line 964, in send\n",
      "    self.connect()\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\http\\client.py\", line 1392, in connect\n",
      "    super().connect()\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\http\\client.py\", line 936, in connect\n",
      "    (self.host,self.port), self.timeout, self.source_address)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\socket.py\", line 722, in create_connection\n",
      "    raise err\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\socket.py\", line 713, in create_connection\n",
      "    sock.connect(sa)\n",
      "socket.timeout: timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\fake_useragent\\utils.py\", line 67, in get\n",
      "    context=context,\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\urllib\\request.py\", line 223, in urlopen\n",
      "    return opener.open(url, data, timeout)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\urllib\\request.py\", line 526, in open\n",
      "    response = self._open(req, data)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\urllib\\request.py\", line 544, in _open\n",
      "    '_open', req)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\urllib\\request.py\", line 504, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\urllib\\request.py\", line 1361, in https_open\n",
      "    context=self._context, check_hostname=self._check_hostname)\n",
      "  File \"C:\\Users\\Administrator\\Anaconda3\\envs\\nlp\\lib\\urllib\\request.py\", line 1320, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error timed out>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\fake_useragent\\utils.py\", line 154, in load\n",
      "    for item in get_browsers(verify_ssl=verify_ssl):\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\fake_useragent\\utils.py\", line 97, in get_browsers\n",
      "    html = get(settings.BROWSERS_STATS_PAGE, verify_ssl=verify_ssl)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python36\\site-packages\\fake_useragent\\utils.py\", line 84, in get\n",
      "    raise FakeUserAgentError('Maximum amount of retries reached')\n",
      "fake_useragent.errors.FakeUserAgentError: Maximum amount of retries reached\n"
     ]
    }
   ],
   "source": [
    "get_pagets_session = FuturesSession(executor=ThreadPoolExecutor(max_workers=10))\n",
    "\n",
    "ua = UserAgent()  #随机生成用户代理\n",
    "\n",
    "def get_page(url='http://www.woshipm.com/__api/v1/stream-list/page/',page_num=4053):\n",
    "    url=url\n",
    "    headers ={\"User-Agent\": ua.random}\n",
    "    url_list=[]\n",
    "    for i in tqdm(range(page_num)):\n",
    "        URL=url+str(i)\n",
    "        #r = requests.get(URL)\n",
    "        r = requests_session.get(URL)\n",
    "        pattern = re.compile(r'http?:\\\\/\\\\/www.woshipm.com\\\\/\\w+\\\\/\\d+.html')\n",
    "        urls = re.findall(pattern,r.result().text)\n",
    "        url_list.append(urls)  \n",
    "    url_list = _flatten(url_list)  #压平嵌套的list\n",
    "    url_list =[i.replace('\\\\','') for i in  url_list]  #去掉链接中的\"\\\\\"符号\n",
    "    url_list =[i for i in url_list if i.find('active')<0]\n",
    "    print('共解析出',len(url_list),'个文章链接')\n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
